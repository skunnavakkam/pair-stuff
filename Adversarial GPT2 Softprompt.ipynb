{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIr3ZNmedIg4",
    "outputId": "09af3389-9c9b-48f7-bcb1-cffff05f47a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudarshanagopalkunnavakkam/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-0.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## Updated by gavento based on code by Sudarsh\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2-0.5B\", device=device)\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PR7dgCGc9_u",
    "outputId": "1f665d4e-1728-4829-bd1b-c6103f63748c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16]) tensor([[   18,    21,    15,    11,   979,   498,  2299,   304,   279, 17846,   653,   498,  1075,  1128,   498,  1490]], device='mps:0')\n",
      "['3', '6', '0', ',', 'Ġwhen', 'Ġyou', \"'re\", 'Ġin', 'Ġthe', 'Ġmirror', 'Ġdo', 'Ġyou', 'Ġlike', 'Ġwhat', 'Ġyou', 'Ġsee']\n",
      "torch.Size([1, 16, 896])\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "input_text = '360, when you\\'re in the mirror do you like what you see'\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = model.to_tokens(input_text)[:, 1:] # Skip BOS\n",
    "print(tokens.shape, tokens)\n",
    "print(model.tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "\n",
    "# Get regular embeddings\n",
    "reg_embeddings = model.embed(tokens)\n",
    "print(reg_embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "SOFT_TOKENS = 1\n",
    "MODEL_DIM = 896\n",
    "L2REG = 0.5\n",
    "\n",
    "def exec_model(model, first_tokens_embedding, tokens):\n",
    "    residual, _tks, _spe, _attn = model.input_to_embed(tokens)\n",
    "\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    both = torch.concat([first_tokens_embedding, residual[:, skip-1:-1, :]], axis=1)\n",
    "    #print(first_tokens_embedding.shape, residual.shape, both.shape)\n",
    "    return model(both, start_at_layer=0)\n",
    "\n",
    "def predict(model, first_tokens_embedding, num_tokens):\n",
    "    tokens = []\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    for i in range(num_tokens):\n",
    "        toks = torch.tensor(tokens, dtype=torch.long)\n",
    "        residual, _tks, _spe, _attn = model.input_to_embed(toks)\n",
    "        both = torch.concat([first_tokens_embedding.detach(), residual], axis=1)\n",
    "        res = model(both, start_at_layer=0)[0]\n",
    "        next_token = torch.argmax(res, axis=-1)[-1].item()  # Get the last token predicted\n",
    "        tokens.append(next_token)\n",
    "        probs = torch.softmax(res, axis=-1)\n",
    "        #print(model.tokenizer.convert_ids_to_tokens(tokens))\n",
    "        #print(residual.shape, toks.shape, both.shape, res.shape, probs.shape)\n",
    "    \n",
    "    return (model.tokenizer.decode(tokens),\n",
    "            model.tokenizer.convert_ids_to_tokens(tokens),\n",
    "            torch.tensor(tokens).cpu())  # Convert the final token list to a tensor\n",
    "\n",
    "def token_alignment_loss(logits, tokens, first_tokens_embedding, alpha=1.0, beta=0.7, gamma=0.1):\n",
    "    # Calculate the cross-entropy loss\n",
    "    def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, tokens)\n",
    "    \n",
    "    # Calculate the token matching loss\n",
    "    pred_tokens = torch.argmax(logits, dim=-1)\n",
    "    match_loss = (pred_tokens != tokens).float().mean()\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_loss = l2(first_tokens_embedding)\n",
    "    \n",
    "    # Combine the losses\n",
    "    total_loss = alpha * match_loss + beta * ce_loss + gamma * l2_loss\n",
    "    \n",
    "    return total_loss\n",
    "# \n",
    "# def nearest_neighbour_loss(x): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cIo3gGq3An",
    "outputId": "3adf9e89-092c-46a6-8437-08a6a6929c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Step 0, Correct=0, Loss=6.224291801452637/0, L2=1.2750376462936401, LR=[0.019979233073936713], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...\\n...\\n...\\n\\n...\\n\\n...\\n...\\n...\\n...\\n'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "\n",
      "Step 10, Correct=0, Loss=9.398530006408691/0, L2=2.5316247940063477, LR=[0.019772746105942783], Pred='.nelelelelelelelelelelelelelelelelelelele'\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "\n",
      "Step 20, Correct=0, Loss=7.612310409545898/0, L2=3.1040899753570557, LR=[0.01956839319723899], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "ADDING NEW TOK\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "\n",
      "Step 30, Correct=0, Loss=7.127480983734131/0, L2=4.17234992980957, LR=[0.01979329840416818], Pred='entiation of the set of all $n\\\\times n$ matrices with real entries is a Ban'\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "\n",
      "Step 40, Correct=0, Loss=5.959875583648682/0, L2=4.637292385101318, LR=[0.019588733085822325], Pred=' entropy of a random variable\\n\\nLet $X$ be a random variable with a finite support. Let'\n",
      "ADDING NEW TOK\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, Correct=0, Loss=6.5568060874938965/0, L2=5.282663345336914, LR=[0.01979329840416818], Pred=' is a measure of the amount of information that can be extracted from a given set of data. In'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Replace the first token's embedding in the reg_embeddings tensor\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mexec_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_tokens_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#print(logits.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)[:last_corr\u001b[38;5;241m+\u001b[39mlossahead], tokens\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)[:last_corr\u001b[38;5;241m+\u001b[39mlossahead]) \u001b[38;5;241m+\u001b[39m L2REG \u001b[38;5;241m*\u001b[39m l2(first_tokens_embedding)\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mexec_model\u001b[0;34m(model, first_tokens_embedding, tokens)\u001b[0m\n\u001b[1;32m     23\u001b[0m both \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([first_tokens_embedding, residual[:, skip\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(first_tokens_embedding.shape, residual.shape, both.shape)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_at_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:550\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    547\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    548\u001b[0m         )\n\u001b[0;32m--> 550\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:162\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    153\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    155\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    160\u001b[0m         query_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(query_input)\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[0;32m--> 162\u001b[0m         key_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m    164\u001b[0m         value_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m    165\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m    166\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/pair stuff/venv/lib/python3.11/site-packages/transformer_lens/components/rms_norm_pre.py:33\u001b[0m, in \u001b[0;36mRMSNormPre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64]:\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     32\u001b[0m scale: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale(\n\u001b[0;32m---> 33\u001b[0m     (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_normalized(x \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "first_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, SOFT_TOKENS, MODEL_DIM)))\n",
    "if device == \"mps\":\n",
    "    first_tokens_embedding = first_tokens_embedding.to(torch.float32).to(device)\n",
    "\n",
    "first_tokens_embedding = first_tokens_embedding.requires_grad_(True)\n",
    "\n",
    "num_steps = 5100  # Number of optimization steps\n",
    "# Define the optimizer for the first tokens' embedding\n",
    "optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "last_corr = 0\n",
    "lossahead = 7\n",
    "lookahead = 20\n",
    "for step in range(num_steps):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Replace the first token's embedding in the reg_embeddings tensor\n",
    "    logits = exec_model(model, first_tokens_embedding, tokens)\n",
    "    #print(logits.shape)\n",
    "    # Calculate the loss\n",
    "    # loss = F.cross_entropy(logits.flatten(0, 1)[:last_corr+lossahead], tokens.flatten(0, 1)[:last_corr+lossahead]) + L2REG * l2(first_tokens_embedding)\n",
    "\n",
    "    flattened_logits = logits.flatten(0, 1)[:last_corr+lossahead]\n",
    "    flattened_tokens = tokens.flatten(0, 1)[:last_corr+lossahead]\n",
    "\n",
    "    loss = token_alignment_loss(flattened_logits, flattened_tokens, first_tokens_embedding)\n",
    "    \n",
    "    # Backpropagate the loss with retain_graph=True\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the first token's embedding\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    # Print the loss for every 100 steps\n",
    "    print(step)\n",
    "    if step % 10 == 0:\n",
    "        ps = predict(model, first_tokens_embedding, last_corr+lookahead)\n",
    "        temp_corr = (ps[2][:min(last_corr+lookahead, tokens.shape[1])] == tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "        if temp_corr > last_corr:\n",
    "            last_corr += 1\n",
    "        print(f\"\\nStep {step}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, L2={l2(first_tokens_embedding).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}\")\n",
    "    \n",
    "    # just comment out this part if you want to not add tokens\n",
    "    # if step % 20 == 0 and step != 0:\n",
    "    #     new_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, 1, MODEL_DIM)), dtype=torch.float32, requires_grad=True).to(device)\n",
    "    #     print(\"ADDING NEW TOK\")\n",
    "    #     first_tokens_embedding = torch.cat([ new_tokens_embedding, first_tokens_embedding], dim=1)\n",
    "    #     first_tokens_embedding = first_tokens_embedding.detach().requires_grad_(True)\n",
    "\n",
    "    #     # Reinitialize the optimizer with the updated embedding\n",
    "    #     optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauI6iQcA3x5",
    "outputId": "f5904061-a6d0-471f-bbd2-a23a818aea05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.0150e-04, -1.1581e-02, -3.2306e-02, -1.3725e-02, -2.2609e-02, -8.8896e-03, -1.9505e-02,  1.6679e-02,  2.7680e-04,  1.2192e-02, -2.7259e-02,  2.4730e-02,  2.8125e-03, -1.9682e-02,  7.6228e-03, -1.2064e-02, -1.2700e-02, -1.0188e-02, -2.9960e-02,  5.0544e-03, -1.8981e-02,  7.1290e-03, -3.6753e-02,  8.2953e-03,  1.0060e-02, -1.9799e-03, -1.0252e-02,  1.0263e-02,  3.0543e-02,  1.5026e-02, -2.7516e-03, -1.1322e-02, -2.3023e-02,  3.3208e-02,  9.7001e-03, -4.6102e-03, -1.4564e-03,  9.0939e-03,  2.7024e-02,  4.2478e-02, -3.4492e-03, -1.5862e-02, -2.0506e-02,  2.7238e-02,  9.4534e-03,  4.7789e-03, -9.4079e-03,  3.8492e-03, -8.3462e-03,  1.3931e-02,  8.5289e-03,  1.6567e-02,  5.5420e-03,  1.0487e-02,  1.8013e-02, -1.1775e-02, -2.1005e-03,  1.3054e-02,  4.5922e-03, -4.3402e-02, -1.8127e-02, -1.3480e-02,  2.7587e-03, -1.9453e-04,  1.3395e-02, -1.3129e-02,  1.1820e-02,  5.8265e-03,  2.0391e-03,  2.7841e-03,  2.7364e-02,  2.8516e-02, -2.6090e-02, -2.7527e-02, -1.6210e-03, -1.1307e-02, -2.0605e-02,  2.5037e-02,  3.2149e-03, -1.1078e-05, -2.3947e-02,  9.1451e-03,  7.8293e-03, -3.0856e-02,  1.7266e-03, -3.4595e-02, -5.6988e-04,  1.7388e-02, -1.8241e-03, -1.2334e-02, -2.9332e-02,  1.2318e-02, -1.8761e-02, -3.0406e-02, -5.7856e-03,  7.2071e-03, -4.4208e-03,  5.3572e-03,  2.3559e-02,  3.4033e-02,  4.4174e-03, -3.3057e-02, -1.5620e-02, -2.6378e-02,  1.0808e-02,  8.1072e-03, -5.7824e-03, -3.8183e-02,  7.7486e-03, -3.5420e-02, -1.3537e-02, -1.0435e-03, -2.3578e-02, -3.0139e-02,  6.8575e-04, -1.5387e-03,  1.0079e-02,  1.7270e-02, -5.6909e-03,  3.5740e-03,  1.1438e-02, -2.2463e-02,  1.8980e-02, -2.4315e-02, -1.9688e-02,  8.8584e-03,  1.2155e-02, -2.7981e-02, -1.9604e-02, -3.7528e-03, -1.1009e-02, -5.6322e-03,  7.7145e-03, -2.7392e-03,  5.3075e-03, -1.0224e-04, -1.9037e-02,  1.6061e-02, -2.1308e-02, -6.5178e-03, -2.1400e-02, -8.5527e-03,  9.4064e-03,  1.2886e-02, -9.2262e-03, -3.0782e-02,  6.6394e-03,  1.6447e-03,  1.6351e-02, -1.6086e-02, -1.5932e-02,  1.9032e-02, -1.5233e-03, -4.4553e-03,  3.8808e-02, -9.2802e-03, -4.4486e-06, -2.4031e-02, -2.4219e-02,  4.4039e-03,  1.1543e-02,  3.5178e-02, -8.7104e-03,  1.1935e-02, -8.5496e-03,  2.1835e-02,  1.3620e-03,  1.6719e-02,  1.8751e-02,  1.7558e-02,  6.8521e-03,  2.1781e-02,  4.7511e-03, -4.9913e-03,  2.7585e-02, -2.1717e-02,  6.9082e-03,  1.5836e-03, -2.3259e-02,  7.1436e-03, -2.7828e-03,  2.2724e-02, -1.3299e-02, -2.0323e-03, -1.3775e-02, -2.7018e-02, -7.9177e-03,  1.2683e-02,  5.6309e-03, -1.6740e-02,  2.3330e-03, -1.6241e-02,  2.0447e-02,  5.3909e-03,  3.5792e-02,  2.6738e-02,  5.7245e-03,  7.2397e-03,  1.9451e-02,  1.0920e-02,  5.9646e-02, -4.3688e-03, -1.9773e-02,  1.2227e-02, -2.3584e-02,  1.5135e-02, -9.0619e-03,  4.2276e-02, -1.9093e-02, -1.0029e-02, -9.1467e-03,  3.2083e-02,  1.0601e-02, -1.5916e-02, -7.1091e-03,  1.9185e-02,  5.7199e-03,  1.2818e-04,  5.2734e-03,  1.1507e-02, -1.6264e-03,  1.7227e-02, -2.1125e-03, -1.4566e-02, -1.4505e-04, -1.1907e-02, -6.2846e-03, -1.8799e-02, -5.1765e-03, -4.3545e-03, -3.9866e-02,  1.0786e-02, -2.8196e-02, -3.4614e-02, -1.8111e-03,  9.1061e-03,  3.1224e-03, -5.9665e-03,  1.5965e-02, -1.9404e-02,  2.0442e-02,  2.5439e-02, -1.0102e-02, -1.0608e-02,  8.6904e-03, -1.7195e-02,  1.4418e-02, -1.2187e-03,  5.1772e-03,  1.6685e-02, -3.2300e-03,  3.6615e-02, -1.2443e-02,  1.6727e-02, -5.5295e-03,  2.9005e-02, -4.6066e-03,  2.6056e-02,  9.0240e-03,  9.5938e-03, -1.7878e-02, -3.5849e-02, -3.1324e-02, -1.7279e-02, -4.5111e-03,  2.5258e-02,  2.3161e-02, -2.4354e-02,  2.7165e-02,  1.4694e-02,  4.2780e-03, -5.5406e-03,  1.4289e-02, -3.8884e-03,  1.1324e-02, -1.2685e-02,  5.5598e-03,  5.6774e-04,  4.1545e-03,  2.4587e-03,  7.6114e-03, -5.7695e-03,  4.0152e-03,  1.8596e-02,  6.0067e-03, -1.4170e-02,  1.3130e-02, -3.7445e-03,  7.8861e-03,  5.4930e-03,  6.6915e-03,  2.8477e-04,  1.0938e-02, -1.1299e-02, -2.2376e-03, -4.6351e-03,  9.8136e-03,  1.5591e-02,  3.3350e-02, -1.7746e-02, -1.1487e-02, -1.3085e-02, -3.9854e-03, -1.4192e-03, -5.1531e-03,  2.8453e-04, -8.8314e-03, -1.0466e-02,  1.5211e-02, -2.6046e-02,  7.8475e-04,  5.0444e-03, -1.9950e-02,  7.1382e-03,  2.7552e-02,  1.8769e-04, -2.3239e-02,  6.3422e-03, -2.6462e-02, -6.9629e-03,  1.0440e-03, -2.9936e-02, -1.2676e-02, -2.1650e-02,  9.1239e-03, -1.7662e-02,  3.2919e-03,  1.4416e-03, -5.4692e-03, -3.4595e-02,  3.7600e-03, -1.4773e-03, -1.4248e-02,  4.2048e-03, -1.7057e-02, -3.9467e-02, -9.1855e-03,  7.8623e-03, -7.7041e-03,  7.6265e-03,  9.6064e-03, -9.1351e-03,  4.6800e-02,  2.4743e-03, -3.1134e-02, -1.9473e-04,  2.7882e-02,  2.2974e-02, -7.1830e-03, -1.3994e-02,  1.6192e-02, -4.5102e-03,  2.0994e-02, -1.4881e-02,  9.5944e-03,  1.3023e-02,  1.8447e-02, -2.6063e-02,  1.0991e-02,  2.4545e-02, -1.0742e-02, -7.2362e-03, -1.1571e-02,  1.7049e-02,  1.4139e-02, -1.1969e-03, -6.9496e-03, -5.0141e-03, -2.3875e-02,  1.4957e-02,  1.8109e-02,  2.4766e-02, -4.3525e-02,  9.1809e-03,  2.3104e-02,  1.0165e-02, -2.6062e-02, -2.8864e-03, -1.3009e-02, -2.6172e-02,  1.5944e-02, -8.9957e-03, -1.0354e-02, -1.9586e-02, -6.4353e-03, -3.4680e-03, -3.3341e-03,  1.6111e-03,  7.3082e-03, -2.7593e-02, -1.9463e-02,  1.3837e-02,  8.6189e-03,  3.8631e-02,  1.1086e-03, -2.6007e-02,  5.2005e-02, -7.2365e-03,  8.0414e-03,  8.5610e-03,  3.4893e-02,  2.6820e-02, -1.3947e-02, -1.3583e-03, -9.0876e-03, -1.1157e-02, -1.7058e-03,  1.3367e-02, -9.6709e-04,  2.4094e-02,  1.4745e-02, -1.4310e-02,  7.8580e-03,  2.5542e-02, -1.8002e-03, -7.1074e-03,  3.3435e-02,  8.3056e-03,  1.7235e-02,  2.8904e-02,  5.2011e-03, -8.3881e-03,  4.7992e-03,  4.7053e-03, -5.0230e-03, -1.5881e-03,  4.5289e-03, -1.2259e-02, -1.5590e-03, -1.0399e-02,  1.2871e-02, -8.5049e-03, -2.8662e-02, -1.1313e-02,  2.2621e-02,  1.0676e-02, -1.2025e-02,  1.0635e-02, -1.1328e-02,  9.4659e-03, -1.3174e-02, -1.0307e-02, -9.3218e-03,  4.6421e-04, -3.1704e-03,  5.5545e-03, -3.8631e-03, -2.3821e-02,  1.5977e-02, -2.6244e-02, -1.0142e-02, -1.7573e-02, -2.6632e-02, -4.4694e-05,  1.5661e-02, -3.3259e-03, -1.6153e-02,  6.2557e-03,  1.2213e-02,  9.5503e-03,  2.9283e-02, -2.9580e-03,  8.0563e-03,  5.3543e-03, -9.1310e-03, -5.0234e-03,  3.0535e-02, -3.2116e-02, -5.5934e-03,  5.1700e-04, -6.0566e-03,  1.9910e-02, -1.3549e-02,  2.3752e-02, -1.6126e-02, -1.1708e-02,  1.2071e-02, -8.6207e-03,  2.2882e-03,  4.8789e-03,  2.9451e-02, -4.1551e-03,  1.0738e-02,  2.9922e-03,  1.2999e-02, -8.0979e-03,  1.6316e-02, -8.0709e-03, -1.0196e-02,  2.1536e-02,  1.3263e-02, -7.2681e-03,  2.2761e-02, -1.1399e-02,  2.9906e-02,  1.4883e-02,  1.6944e-02,  1.5840e-02, -1.0503e-02,  9.4957e-03,  2.2880e-02,  1.0177e-02, -6.2969e-03, -9.6932e-04,  1.4020e-02,  4.5169e-03, -1.7763e-02,  2.4220e-02, -2.0459e-02, -1.2915e-02, -6.1743e-03,  3.5032e-03, -7.9352e-02, -1.9932e-02, -2.8390e-02, -5.0723e-03, -3.4714e-03,  5.4369e-03, -8.9914e-03,  1.8070e-02,  2.3226e-03,  2.3709e-03,  1.0419e-02, -1.2094e-02,  1.1300e-02,  3.9284e-03, -9.5982e-03, -9.7351e-04,  1.4661e-02, -6.2686e-03,  1.2426e-02,  1.0637e-02,  2.1318e-02, -9.3136e-04, -1.3743e-02,  1.0151e-02, -9.4921e-03,  2.0236e-02, -2.0062e-02, -5.1929e-03, -1.7561e-02, -1.6140e-03,  4.4213e-03,  8.3560e-03, -5.5550e-03,  8.5080e-03, -5.8721e-03, -1.2080e-02,  1.4825e-02, -1.7086e-03,  6.2252e-03, -6.3918e-03, -1.6075e-02, -1.5325e-02, -3.5543e-03, -9.9339e-03,  1.0567e-02, -2.7212e-02, -2.5554e-02,  2.0057e-02,  4.1308e-03,  6.9440e-03,  3.3333e-03, -9.3572e-03,  1.7505e-02, -4.3612e-03, -1.8630e-02,  1.2036e-02, -2.0417e-02, -9.3942e-03, -6.6462e-03,  2.2612e-02,  2.5780e-02,  9.9162e-03, -2.1364e-02,  2.7868e-02, -1.9987e-02,  1.7424e-02,  4.4396e-02, -6.9290e-03, -1.6016e-03, -3.0039e-03, -8.2343e-03, -3.0194e-03,  2.6933e-02, -7.1304e-04,  6.2029e-03, -3.5033e-03, -9.2667e-03,  1.6687e-03, -1.8548e-02,  3.5297e-02, -8.5466e-03, -9.3698e-03,  1.5509e-02,  6.7087e-03, -1.5980e-02,  4.4468e-03,  7.8201e-03,  7.5963e-03,  1.1762e-02, -5.4033e-03, -4.8174e-03,  1.6323e-02, -1.3569e-02,  2.0915e-02, -1.6364e-02, -1.6226e-02,  1.4548e-02, -1.5039e-02,  7.0112e-03, -2.9950e-03,  2.2404e-02,  1.7648e-02, -1.5475e-02,  3.8688e-03,  5.8316e-03,  1.2483e-02, -1.2515e-02, -2.7047e-02, -1.1819e-03,  2.5894e-02,  1.2923e-02, -2.7638e-03,  7.8514e-03,  4.7312e-03, -2.2035e-02,  1.1182e-03,  1.2883e-03,  3.7134e-02,  3.6051e-02, -8.3572e-03,  1.9322e-02, -1.0407e-02, -9.8187e-03,  3.3213e-02, -2.1898e-03,  8.5114e-03, -2.0462e-02,  4.0460e-02,  1.4571e-02, -2.6842e-02, -2.4878e-02, -1.0134e-02, -8.5364e-03, -6.4351e-03,  4.3292e-03, -2.0477e-02,  4.8648e-03,  5.9584e-03,  7.7966e-03, -1.4187e-02, -1.2376e-02, -2.0563e-02, -1.8598e-02,  2.5361e-03, -5.0360e-02,  6.0994e-04, -1.6581e-02, -3.5872e-02, -1.4403e-02, -1.7203e-02, -3.2146e-02,  9.3535e-04, -1.1667e-02, -1.5403e-02, -1.8480e-02,  1.1126e-02, -8.6138e-03, -5.0793e-03, -1.6027e-02, -8.8371e-04,  1.9139e-02,  2.0148e-02,  2.6569e-03, -1.0733e-02,  7.7832e-03,  6.7002e-03, -2.6456e-02,  1.6181e-02, -6.7256e-03,  3.7615e-02, -1.9125e-03,  1.8859e-02,  1.5763e-02,  1.2028e-02,  3.3293e-02,  6.6003e-03,  2.1927e-03,  1.7036e-02, -2.0755e-03,  1.4152e-02, -2.8863e-02,  3.4540e-02, -1.1953e-03,  4.1939e-03,  3.0212e-03, -1.5845e-02, -3.5725e-03,  9.6461e-03, -8.4756e-03,  5.4755e-03,  1.1785e-02,  1.5466e-02, -2.7090e-02,  1.6697e-02,  1.4516e-02, -9.4305e-03, -6.1746e-03, -2.3857e-02, -2.8528e-02,  9.3079e-03, -1.3087e-02,  2.0581e-02,  1.5455e-02, -1.1556e-02, -1.2936e-02, -1.2885e-02, -1.0575e-02, -5.1464e-03,  1.1446e-02, -2.7188e-03,  1.3069e-02, -1.9484e-02,  2.1922e-02,  1.1345e-02,  2.4466e-02,  6.2379e-03,  1.2512e-04,  1.3113e-02,  2.3272e-03, -2.5278e-02,  1.0376e-02,  4.8013e-03, -3.3845e-03,  8.5018e-03, -6.5116e-03,  2.2161e-02, -2.2682e-02,  2.2616e-03, -2.6205e-02,  1.7038e-02,  1.4647e-02,  2.3288e-02, -4.0197e-03, -1.0576e-02, -2.2771e-02, -5.5391e-04, -2.4368e-02, -2.0355e-02,  8.2951e-03, -1.4498e-02, -1.1257e-02, -1.9697e-03,  1.2621e-02, -8.2458e-04, -1.1902e-02,  3.2632e-02, -2.4777e-02,  2.1431e-02,  7.5719e-03, -7.6557e-03, -1.5156e-02, -2.2749e-02,  9.5315e-03, -6.3532e-04,  2.0779e-02, -1.5976e-03]]], device='mps:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/sjbgw9357l188k3xlvvxhwyc0000gn/T/ipykernel_14538/73013378.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  toks = torch.tensor(tokens, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterising order, such as and independently, such, such, both, both, both, both, both, both, both, both, both, both, both, and both, and both, and both, and both, and both, and both,'\n",
      "'Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.'\n"
     ]
    }
   ],
   "source": [
    "# The prediction runs only given the following tensor shaped (1,1,768):\n",
    "print(first_tokens_embedding)\n",
    "MAGIC = torch.tensor([[[ 0.0722,  0.0018, -0.0332,  0.0840,  0.0088,  0.0193, -0.0776,  0.0729,  0.0186, -0.0036, -0.0359,  0.1284, -0.0428,  0.0698,  0.0470, -0.0241, -0.0458, -0.0276,  0.0031, -0.0530, -0.1170, -0.0169, -0.0305,  0.0401, -0.0561, -0.0567,  0.0276,  0.0757, -0.0233,  0.1150, -0.0454,  0.0203,  0.0384,  0.0577, -0.0058, -0.0583,  0.0537, -0.0087, -0.0267, -0.0287,  0.0393, -0.0766, -0.0212, -0.0063, -0.0931,  0.0343, -0.1035,  0.0565,  0.0423,  0.0126,  0.0722, -0.0143, -0.0060,  0.0589,  0.0825, -0.0253, -0.0061,  0.0526, -0.0367,  0.0349, -0.0842, -0.0090,  0.0014,  0.0716,  0.0752, -0.0421,  0.0591, -0.0066,  0.0444,  0.0431,  0.0639, -0.0387,  0.0112, -0.0076,  0.1020,  0.0308, -0.1242,  0.0220, -0.1179, -0.0400, -0.0526,  0.0353, -0.0109, -0.0455, -0.0189,  0.0022,  0.0773,  0.0767, -0.0153,  0.0591, -0.0570,  0.0563, -0.0706, -0.0480,  0.0171, -0.0015, -0.0300, -0.0241,  0.0291, -0.0339, -0.0071,  0.0210, -0.0025, -0.0055,  0.0188,  0.0200, -0.0551,  0.0525,  0.0311, -0.0937, -0.0839, -0.1068,  0.0302, -0.0122,  0.0020, -0.0559, -0.0078, -0.0015, -0.0401,  0.0463, -0.0237,  0.0726,  0.1067, -0.0347, -0.0233,  0.0067,  0.0091, -0.0825,  0.0250,  0.0064,  0.0545, -0.0087, -0.0274,  0.1103,  0.0077,  0.0357, -0.0424,  0.0451, -0.1372, -0.0541,  0.0402,  0.0057,  0.0520,  0.0205,  0.0240, -0.0587,  0.0458,  0.0314,  0.0432,  0.0024, -0.0410,  0.0260, -0.0278, -0.0438,  0.0320, -0.0563, -0.0502,  0.0074, -0.0817,  0.0423,  0.0250, -0.0064,  0.0055,  0.0313,  0.0897,  0.0786, -0.0028,  0.0643,  0.0133,  0.0792, -0.0145, -0.0455,  0.0045,  0.0213,  0.0275,  0.0145, -0.0258, -0.0755, -0.0116,  0.0197,  0.0470, -0.0054, -0.0071, -0.0756, -0.0021, -0.0248,  0.0113, -0.0651,  0.1203, -0.1215, -0.0508,  0.0435, -0.0896, -0.0312,  0.0862,  0.0694,  0.0316,  0.0801, -0.0263,  0.0139,  0.0740, -0.0926,  0.0364,  0.0207, -0.0460,  0.0351,  0.0083,  0.0178,  0.0173, -0.0611,  0.0472,  0.0112, -0.0603, -0.0213, -0.1077,  0.0311, -0.0408, -0.0207, -0.0425, -0.0180,  0.0130, -0.0963,  0.0482,  0.0084, -0.0331,  0.0235, -0.0006, -0.0231, -0.0599, -0.0082, -0.0876,  0.0229, -0.1219, -0.0388,  0.0745, -0.0089,  0.0047, -0.0055, -0.0821, -0.0338, -0.0043, -0.0576,  0.0157,  0.0211,  0.0435, -0.0266, -0.0077,  0.0518, -0.0055, -0.0808,  0.0975, -0.0411,  0.0157,  0.0038,  0.0071,  0.0017, -0.0042, -0.0058,  0.0274, -0.1161, -0.0185,  0.0205,  0.1240, -0.0251,  0.0989,  0.0354,  0.0817, -0.0644, -0.0352, -0.1421, -0.0112, -0.0024, -0.0371, -0.0382,  0.0332, -0.0662, -0.0356,  0.0206, -0.1179, -0.0053, -0.0148, -0.0397,  0.0005,  0.0112,  0.0105,  0.0260, -0.0227,  0.0138,  0.0317, -0.0067,  0.0022,  0.0060, -0.0015, -0.0391, -0.0102, -0.0430,  0.0620, -0.0184,  0.0033,  0.0062, -0.0511,  0.0313,  0.0205, -0.0378, -0.0725, -0.0652,  0.0188,  0.0165,  0.0161, -0.0728,  0.0189,  0.0626, -0.0146, -0.0428,  0.1350,  0.0410, -0.0336,  0.1037, -0.0491,  0.0251, -0.0151, -0.0277, -0.0178, -0.0020,  0.0371, -0.0144,  0.0894,  0.0304, -0.0409,  0.1606,  0.0453, -0.0809,  0.0031, -0.0099,  0.0288, -0.0419, -0.0154, -0.0242, -0.0344,  0.0128,  0.0481, -0.0133,  0.0081, -0.0493, -0.0442,  0.0301,  0.0068, -0.0065,  0.0467, -0.0127,  0.0164, -0.0336, -0.0299, -0.1080,  0.0640,  0.0241,  0.0609,  0.0010, -0.1020,  0.1194,  0.1178,  0.0152, -0.0918, -0.0748,  0.0845, -0.0168,  0.0327, -0.0258, -0.0150, -0.0381, -0.0359,  0.0518, -0.0464,  0.0771,  0.0054,  0.0455, -0.0223,  0.0444,  0.0843, -0.0550, -0.0277, -0.0142,  0.0248, -0.0285, -0.0420, -0.0065,  0.0117, -0.0557, -0.0032, -0.1257, -0.0286,  0.0048, -0.0523,  0.0255,  0.0799,  0.0364,  0.0957, -0.0922, -0.0183,  0.0339,  0.0504,  0.0726,  0.0269, -0.0327,  0.0941,  0.0243, -0.0040,  0.0495, -0.0269,  0.0651,  0.0257,  0.0104, -0.0555,  0.0147,  0.0665, -0.0048, -0.0580,  0.0178,  0.0786,  0.0258, -0.0353,  0.0252,  0.0546,  0.0195, -0.0110, -0.0478,  0.0180,  0.0638,  0.0701,  0.0045,  0.0950,  0.0025, -0.0315, -0.0307,  0.0334, -0.0189, -0.0133, -0.0390, -0.0344,  0.0437, -0.0925,  0.0021,  0.0377,  0.0144, -0.0829, -0.0185,  0.0589, -0.0989, -0.0132, -0.0768,  0.0231,  0.0173, -0.0084, -0.0252, -0.0269, -0.0345, -0.0033,  0.0335,  0.0022,  0.0436,  0.0678, -0.0262,  0.0217,  0.0203, -0.0251, -0.0531, -0.0335, -0.0079,  0.0427,  0.0197, -0.0349,  0.0027,  0.0525, -0.0131, -0.0187, -0.1100,  0.0276, -0.0469, -0.0280,  0.0255,  0.0962, -0.0727,  0.0039, -0.0969,  0.0008,  0.0315,  0.0379,  0.0419, -0.0900, -0.0674,  0.0666,  0.0287,  0.0408, -0.0418,  0.0102,  0.0461, -0.0309,  0.0791, -0.0369,  0.1147, -0.0701, -0.0152,  0.0740,  0.0453, -0.0131, -0.0309,  0.0264,  0.0032,  0.0235, -0.1436,  0.0531, -0.0129, -0.0275, -0.0223,  0.0008, -0.0111,  0.0566,  0.0777, -0.0393,  0.0559,  0.0766,  0.1292, -0.0004,  0.1153,  0.0237, -0.0440, -0.0127,  0.0600,  0.0339,  0.0036, -0.0151, -0.0314,  0.0866, -0.0226, -0.0160, -0.0388, -0.0165,  0.0115,  0.0944, -0.0367, -0.0420, -0.0094, -0.0357, -0.0448, -0.0329,  0.0900,  0.0505, -0.0280,  0.0869,  0.0205, -0.0039, -0.0045,  0.0231,  0.0936, -0.0189, -0.0244,  0.0386, -0.0084, -0.0992, -0.0358, -0.0255, -0.0109, -0.0223,  0.0658,  0.0210, -0.0090,  0.0513,  0.0648, -0.0428,  0.0203, -0.0385,  0.0421, -0.0428, -0.0597, -0.0281,  0.1150, -0.0488, -0.0213,  0.0962,  0.0037, -0.0588, -0.0330, -0.0141,  0.0646, -0.0179, -0.0203, -0.0204,  0.0413, -0.0339,  0.0377, -0.0797,  0.0210,  0.0395, -0.0612, -0.0291, -0.0481,  0.0418,  0.0170, -0.0641, -0.0139,  0.0308,  0.0076,  0.0104,  0.0205, -0.0043,  0.0222, -0.0228,  0.0337, -0.0021,  0.0628, -0.0111,  0.0041,  0.0705, -0.0225, -0.0248,  0.0287,  0.0709,  0.0109, -0.0558, -0.0489, -0.0321,  0.0767,  0.0377, -0.0531,  0.0612, -0.0467,  0.0343,  0.0195,  0.0741,  0.0963, -0.0236,  0.0707, -0.0491, -0.0407, -0.0370,  0.0806,  0.0174, -0.0130, -0.0194,  0.0155,  0.0348,  0.0540, -0.0357,  0.0176,  0.0408,  0.0997,  0.0262, -0.0179,  0.0663,  0.0956,  0.0476, -0.0568, -0.0305,  0.0063,  0.0643,  0.0656, -0.0427, -0.1023,  0.1351, -0.0445, -0.0047, -0.0674, -0.0750,  0.0591, -0.0946,  0.0220,  0.0023,  0.0086, -0.0895,  0.0430, -0.0613,  0.0638, -0.0318,  0.0536,  0.0216,  0.0350, -0.0043, -0.0470,  0.0360,  0.0944,  0.0140,  0.0915,  0.0719,  0.0168, -0.0117, -0.0369, -0.0402, -0.0139, -0.0954, -0.0463, -0.0829,  0.0891, -0.0709,  0.0022, -0.0028,  0.0302, -0.1730,  0.0585,  0.0022,  0.0357,  0.0248,  0.0274,  0.0361, -0.0570,  0.0508,  0.0770, -0.0273,  0.0608,  0.0062, -0.0077, -0.0392,  0.0320, -0.0385, -0.0652,  0.0106,  0.0014, -0.0817, -0.1166, -0.0085,  0.0298, -0.0669,  0.0334, -0.0173, -0.0321,  0.0299, -0.0333, -0.0049,  0.0614, -0.0467,  0.0267,  0.0398,  0.0070,  0.0534, -0.0180, -0.0445,  0.0437, -0.0795, -0.0665, -0.0085, -0.0171, -0.0207,  0.0128,  0.0639, -0.0206,  0.0176,  0.0282,  0.0749,  0.0757, -0.0355, -0.0445, -0.0050, -0.0335, -0.0774, -0.0544,  0.0394, -0.0315,  0.0321,  0.0474, -0.0200,  0.0276,  0.0329, -0.0617, -0.0847, -0.0258, -0.0408,  0.0513,  0.0118,  0.0024]]], device='mps', dtype=torch.float32, requires_grad=True)\n",
    "# Compare the following:\n",
    "print(repr(predict(model, MAGIC, 90)[0]))\n",
    "print(repr(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQDhlHdYH_oC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
