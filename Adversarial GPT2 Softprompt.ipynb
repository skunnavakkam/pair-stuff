{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIr3ZNmedIg4",
    "outputId": "09af3389-9c9b-48f7-bcb1-cffff05f47a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## Updated by gavento based on code by Sudarsh\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=device)\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PR7dgCGc9_u",
    "outputId": "1f665d4e-1728-4829-bd1b-c6103f63748c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90]) tensor([[ 6892,   876, 40709,   318,  1464,   257,  1729,    12, 31591,  1103,  1271,    11,   351,  1988,   657,   611,   290,   691,   611,   262,   734, 24570,   287,  1808,   389, 10411,    13,   632,   468, 10084,  5479,    11,  1111, 16200,    11,   884,   355,  2095,  2890,   262,  3585,   357,  2484,  8825,     8, 40709,   287,  1321,  3341,    11,  4738,  1108,   287, 12948,   640,    12, 25076,    11,   290,  1321,  4461,   618, 14176, 13905,  4981,   286, 32278,    26,   290,  8472,    11,   884,   355,  5625,  7869,    11, 11711, 12933,    11, 39738,    11, 13401,   259, 18982,   873,    11,   290,  4572,  4673,    13]], device='cuda:0')\n",
      "['Rel', 'ative', 'Ġentropy', 'Ġis', 'Ġalways', 'Ġa', 'Ġnon', '-', 'negative', 'Ġreal', 'Ġnumber', ',', 'Ġwith', 'Ġvalue', 'Ġ0', 'Ġif', 'Ġand', 'Ġonly', 'Ġif', 'Ġthe', 'Ġtwo', 'Ġdistributions', 'Ġin', 'Ġquestion', 'Ġare', 'Ġidentical', '.', 'ĠIt', 'Ġhas', 'Ġdiverse', 'Ġapplications', ',', 'Ġboth', 'Ġtheoretical', ',', 'Ġsuch', 'Ġas', 'Ġcharacter', 'izing', 'Ġthe', 'Ġrelative', 'Ġ(', 'Sh', 'annon', ')', 'Ġentropy', 'Ġin', 'Ġinformation', 'Ġsystems', ',', 'Ġrandom', 'ness', 'Ġin', 'Ġcontinuous', 'Ġtime', '-', 'series', ',', 'Ġand', 'Ġinformation', 'Ġgain', 'Ġwhen', 'Ġcomparing', 'Ġstatistical', 'Ġmodels', 'Ġof', 'Ġinference', ';', 'Ġand', 'Ġpractical', ',', 'Ġsuch', 'Ġas', 'Ġapplied', 'Ġstatistics', ',', 'Ġfluid', 'Ġmechanics', ',', 'Ġneuroscience', ',', 'Ġbio', 'in', 'format', 'ics', ',', 'Ġand', 'Ġmachine', 'Ġlearning', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "input_text = \"\"\"Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = model.to_tokens(input_text)[:, 1:] # Skip BOS\n",
    "print(tokens.shape, tokens)\n",
    "print(model.tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "\n",
    "# \n",
    "# embeddings_tree = BallTree(W_E.numpy(), leaf_size=1)\n",
    "\n",
    "SOFT_TOKENS = 10\n",
    "MODEL_DIM = 1024\n",
    "L2REG = 0.5\n",
    "\n",
    "def exec_model(model, first_tokens_embedding, tokens):\n",
    "    residual, _tks, _spe, _attn = model.input_to_embed(tokens)\n",
    "\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    both = torch.concat([first_tokens_embedding, residual], axis=1)\n",
    "    #print(first_tokens_embedding.shape, residual.shape, both.shape)\n",
    "    return model(both, start_at_layer=0)\n",
    "\n",
    "def predict(model, first_tokens_embedding, num_tokens):\n",
    "    tokens = []\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    for i in range(num_tokens):\n",
    "        toks = torch.tensor(tokens, dtype=torch.long)\n",
    "        residual, _tks, _spe, _attn = model.input_to_embed(toks)\n",
    "        both = torch.concat([first_tokens_embedding.detach(), residual], axis=1)\n",
    "        res = model(both, start_at_layer=0)[0]\n",
    "        next_token = torch.argmax(res, axis=-1)[-1].item()  # Get the last token predicted\n",
    "        tokens.append(next_token)\n",
    "        probs = torch.softmax(res, axis=-1)\n",
    "        #print(model.tokenizer.convert_ids_to_tokens(tokens))\n",
    "        #print(residual.shape, toks.shape, both.shape, res.shape, probs.shape)\n",
    "    \n",
    "    return (model.tokenizer.decode(tokens),\n",
    "            model.tokenizer.convert_ids_to_tokens(tokens),\n",
    "            torch.tensor(tokens).cpu())  # Convert the final token list to a tensor\n",
    "\n",
    "def nearest_neighbour_loss(x, ball_tree, W_E): \n",
    "    # Get the nearest neighbour\n",
    "    dist, ind = ball_tree.query(x.detach().cpu().numpy(), k=1)\n",
    "    nearest_neighbour = W_E[ind].to(device)\n",
    "\n",
    "    # then we calculate the distance between x and its nearest neighbour\n",
    "    distance = torch.norm(x - nearest_neighbour, p=2)\n",
    "    return distance\n",
    "\n",
    "def token_alignment_loss(logits, tokens, first_tokens_embedding, alpha=1.0, beta=0.7, kappa=0.1, gamma=0.2):\n",
    "    # Calculate the cross-entropy loss\n",
    "    def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "    logits = logits[SOFT_TOKENS - 1:-1, :]\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, tokens)\n",
    "    \n",
    "    # Calculate the token matching loss\n",
    "    # pred_tokens = torch.argmax(logits, dim=-1)\n",
    "    # match_loss = (pred_tokens != tokens).float().mean()\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_loss = l2(first_tokens_embedding)\n",
    "\n",
    "    # # do nearest neighbour loss for each token in first_tokens_embedding\n",
    "    # nn_loss = 0\n",
    "    # for i in range(first_tokens_embedding.shape[1]):\n",
    "    #     nn_loss += nearest_neighbour_loss(first_tokens_embedding[:, i, :], embeddings_tree, W_E)\n",
    "    \n",
    "    # Combine the losses\n",
    "    # total_loss = alpha * match_loss + beta * ce_loss + gamma * l2_loss #+ kappa * nn_loss\n",
    "\n",
    "    total_loss = ce_loss + gamma * l2_loss\n",
    "\n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cIo3gGq3An",
    "outputId": "3adf9e89-092c-46a6-8437-08a6a6929c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Step 0, Maximum Correct=0, Correct=0, Loss=30.06426429748535/0, L2=102.71237182617188, LR=[0.19894313879210052], Pred='.\\n.\\n.\\n.\\n.\\nThe word.The word \"n\" is a'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, Maximum Correct=20, Correct=20, Loss=9.686123847961426/5, L2=45.36923599243164, LR=[0.15264320137581455], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "Step 100, Maximum Correct=25, Correct=25, Loss=8.452168464660645/10, L2=41.23023223876953, LR=[0.1171186252902463], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are'\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "\n",
      "Step 150, Maximum Correct=25, Correct=1, Loss=14.179256439208984/10, L2=58.06599807739258, LR=[0.08986166606992071], Pred='equitable, and the value of the value of the value of the value of the value of the value of the value of the value of the value'\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "\n",
      "Step 200, Maximum Correct=30, Correct=30, Loss=10.238627433776855/15, L2=49.13604736328125, LR=[0.06894820536742106], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse'\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "\n",
      "Step 250, Maximum Correct=32, Correct=32, Loss=6.547300815582275/20, L2=31.03898048400879, LR=[0.05290192393816882], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, and finds its'\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "\n",
      "Step 300, Maximum Correct=40, Correct=40, Loss=3.4472577571868896/25, L2=16.892593383789062, LR=[0.040590085578676736], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the'\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "\n",
      "Step 350, Maximum Correct=40, Correct=5, Loss=8.703407287597656/25, L2=34.10546112060547, LR=[0.031143575216847408], Pred='Relative entropy is a non-negative integer, non-negative integer, non-negative integer, non-negative integer, and non-negative integer, including its variance, is the product of two or more non-negative'\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "\n",
      "Step 400, Maximum Correct=41, Correct=41, Loss=6.034127235412598/29, L2=28.735652923583984, LR=[0.023895546497614755], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative abundance of a given'\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "\n",
      "Step 450, Maximum Correct=49, Correct=49, Loss=5.031396865844727/34, L2=24.993528366088867, LR=[0.018334347885363634], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems'\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "\n",
      "Step 500, Maximum Correct=54, Correct=54, Loss=4.7212371826171875/39, L2=23.453723907470703, LR=[0.014067404251042834], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous'\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "\n",
      "Step 550, Maximum Correct=59, Correct=59, Loss=4.353489875793457/44, L2=21.67023468017578, LR=[0.010793504279486036], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and'\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "\n",
      "Step 600, Maximum Correct=64, Correct=64, Loss=4.139852523803711/49, L2=20.60076332092285, LR=[0.008281537414597796], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical'\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "\n",
      "Step 650, Maximum Correct=69, Correct=69, Loss=3.9879112243652344/54, L2=19.849979400634766, LR=[0.00635417934467609], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and'\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "\n",
      "Step 700, Maximum Correct=74, Correct=74, Loss=3.923384666442871/59, L2=19.517311096191406, LR=[0.004875374356594533], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied'\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "\n",
      "Step 750, Maximum Correct=79, Correct=79, Loss=3.8055214881896973/64, L2=18.958637237548828, LR=[0.0037407309154493533], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics,'\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "\n",
      "Step 800, Maximum Correct=84, Correct=84, Loss=3.731767177581787/69, L2=18.57794189453125, LR=[0.0028701524761624154], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformat'\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "\n",
      "Step 850, Maximum Correct=89, Correct=89, Loss=3.6690237522125244/74, L2=18.275062561035156, LR=[0.0022021833226225766], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning'\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "\n",
      "Step 900, Maximum Correct=90, Correct=90, Loss=3.616239547729492/78, L2=18.028118133544922, LR=[0.0016896702968621593], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.matrix is always'\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "\n",
      "Step 950, Maximum Correct=90, Correct=90, Loss=3.5766220092773438/81, L2=17.837596893310547, LR=[0.0012964341718373653], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.matrix is always finds, information gain'\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ea9963cb650>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "first_tokens_embedding = torch.randn(size=(1, SOFT_TOKENS, MODEL_DIM), dtype=torch.float32, device=device)\n",
    "if device == \"cuda\": \n",
    "    first_tokens_embedding = first_tokens_embedding.cuda()\n",
    "if device == \"mps\":\n",
    "    first_tokens_embedding = first_tokens_embedding.to(torch.float32).to(device)\n",
    "\n",
    "first_tokens_embedding = first_tokens_embedding.requires_grad_(True)\n",
    "\n",
    "num_steps = 1000  # Number of optimization steps\n",
    "# Define the optimizer for the first tokens' embedding\n",
    "optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02 * SOFT_TOKENS, amsgrad=True)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "last_corr = 0\n",
    "max_corr = 0\n",
    "lossahead = 5 + SOFT_TOKENS * 2\n",
    "lookahead = 20\n",
    "kappa = 0.1\n",
    "alpha = 0.0\n",
    "beta = 1.0\n",
    "for step in range(num_steps):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Replace the first token's embedding in the reg_embeddings tensor\n",
    "    logits = exec_model(model, first_tokens_embedding, tokens)\n",
    "    #print(logits.shape)\n",
    "    # Calculate the loss\n",
    "    # loss = F.cross_entropy(logits.flatten(0, 1)[:last_corr+lossahead], tokens.flatten(0, 1)[:last_corr+lossahead]) + L2REG * l2(first_tokens_embedding)\n",
    "\n",
    "    flattened_logits = logits.flatten(0, 1)[:last_corr+lookahead+SOFT_TOKENS]\n",
    "    flattened_tokens = tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    loss = token_alignment_loss(flattened_logits, flattened_tokens, first_tokens_embedding, alpha=0.0, beta=1.0)\n",
    "    \n",
    "    # Backpropagate the loss with retain_graph=True\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the first token's embedding\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    # Print the loss for every 100 steps\n",
    "    print(step)\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():  # Disable gradient computation for prediction\n",
    "\n",
    "            ps = predict(model, first_tokens_embedding, last_corr+lookahead)\n",
    "            temp_corr = (ps[2][:min(last_corr+lookahead, tokens.shape[1])] == tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "            max_corr = max(temp_corr, max_corr)\n",
    "            if temp_corr > last_corr:\n",
    "                last_corr += math.ceil((temp_corr - last_corr) / 4)\n",
    "            print(f\"\\nStep {step}, Maximum Correct={max_corr}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, L2={l2(first_tokens_embedding).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}\")\n",
    "    \n",
    "\n",
    "    # just comment out this part if you want to not add tokens\n",
    "    # if step % 20 == 0 and step != 0:\n",
    "    #     new_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, 1, MODEL_DIM)), dtype=torch.float32, requires_grad=True).to(device)\n",
    "    #     print(\"ADDING NEW TOK\")\n",
    "    #     first_tokens_embedding = torch.cat([ new_tokens_embedding, first_tokens_embedding], dim=1)\n",
    "    #     first_tokens_embedding = first_tokens_embedding.detach().requires_grad_(True)\n",
    "\n",
    "    #     # Reinitialize the optimizer with the updated embedding\n",
    "    #     optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauI6iQcA3x5",
    "outputId": "f5904061-a6d0-471f-bbd2-a23a818aea05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.'\n",
      "'Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.'\n"
     ]
    }
   ],
   "source": [
    "# The prediction runs only given the following tensor shaped (1,1,768):\n",
    "MAGIC = first_tokens_embedding\n",
    "# Compare the following:\n",
    "print(repr(predict(model, MAGIC, 90)[0]))\n",
    "print(repr(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "precious = first_tokens_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 1024]) torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# I use small models because I'm GPU poor\n",
    "model1 = \"gpt2-medium\"\n",
    "model2 = \"gpt2\"\n",
    "\n",
    "# Load the models\n",
    "model1_tokenizer = AutoTokenizer.from_pretrained(model1)\n",
    "model2_tokenizer = AutoTokenizer.from_pretrained(model2)\n",
    "\n",
    "model1_vocab_dict = model1_tokenizer.get_vocab()\n",
    "model2_vocab_dict = model2_tokenizer.get_vocab()\n",
    "model1_vocab = set(model1_vocab_dict.keys())\n",
    "model2_vocab = set(model2_vocab_dict.keys())\n",
    "intersection = list(model1_vocab.intersection(model2_vocab))\n",
    "\n",
    "# now we need to get the embedding matrix for each model\n",
    "model1_model = AutoModelForCausalLM.from_pretrained(model1)\n",
    "model2_model = AutoModelForCausalLM.from_pretrained(model2)\n",
    "\n",
    "model1_embedding_matrix = model1_model.get_input_embeddings().weight.data\n",
    "model2_embedding_matrix = model2_model.get_input_embeddings().weight.data\n",
    "\n",
    "A = model1_embedding_matrix[model1_tokenizer.convert_tokens_to_ids(intersection)]\n",
    "B = model2_embedding_matrix[model2_tokenizer.convert_tokens_to_ids(intersection)]\n",
    "\n",
    "print(A.shape, B.shape)\n",
    "\n",
    "# we need to find the average vector in A\n",
    "A_avg = torch.mean(A, axis=0)\n",
    "B_avg = torch.mean(B, axis=0)\n",
    "\n",
    "# now we subtract the average vector from each vector in A and B\n",
    "A = A - A_avg\n",
    "B = B - B_avg\n",
    "\n",
    "# split A and B into 90, 10\n",
    "A_train = A[: int(0.9 * len(A))]\n",
    "A_test = A[int(0.9 * len(A)) :]\n",
    "B_train = B[: int(0.9 * len(B))]\n",
    "B_test = B[int(0.9 * len(B)) :]\n",
    "\n",
    "# then we need to create a matrix W\n",
    "# then minimizing ||AW - B||^2\n",
    "\n",
    "W, residuals, rank, singular_values = torch.linalg.lstsq(A_train, B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "torch.Size([1, 10, 1024])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# transform precious\n",
    "\n",
    "precious_transformed = precious[0, :, :].unsqueeze(0).to(\"cpu\")\n",
    "precious_transformed = precious_transformed - A_avg\n",
    "precious_transformed = precious_transformed @ W\n",
    "precious_transformed = precious_transformed + B_avg\n",
    "print(precious_transformed.shape)\n",
    "print(precious.shape)\n",
    "\n",
    "print(precious_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "precious_transformed = precious_transformed.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2\", device=device)\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -, in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, precious_transformed, 100)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
