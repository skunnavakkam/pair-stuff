import torch
from transformer_lens import HookedTransformer

first_embedding = torch.tensor([[[  1.0560,   1.1948,   2.7735,   2.4671,  -2.3569,   2.5548,  -1.6624,   2.2987,   2.4258,   4.2382,   0.1023,  -2.2353,   3.6276,  -1.5187,  -1.2637,   2.7799,  -3.3597,   0.8116,  -2.5778,   3.1615,  -2.7673,   2.8104,  -1.5018,   2.1332,   2.2542,  -4.1591,   2.9243,   3.4152,   2.6786,   5.9969,   7.3154,   0.3116,   4.0839,   4.2265,  -2.2327,  -1.8355,   1.4318,   2.6556,   1.4921,   1.7508,  -0.4488,   3.6992,  -1.2894,   2.1225,  -3.0501,  -5.9817,  -4.6665,  -2.8403,  -2.9002,   2.1656,   3.0753,  -2.4790,  -1.7390,   1.4818,  -0.4629,  -3.9446,  -3.2976,   4.0621,  -1.5321,   3.7099,   2.5315,   2.8040,  -2.3071,   3.0088,  -3.0297,  -0.1470,  -6.1246,   5.7057,  -1.8261,   1.6841,  -1.7023,   6.1310,  -2.4765,   3.3615,   2.9706,   3.6894,  -2.6274,  -2.0579,   3.2183,   2.5064,  -2.0239,  -2.0464,  -3.1984,  -1.4849,   1.9731,  -0.8164,  -4.9004,  -2.9546,  -2.6237,  -4.3184,  -1.7252,   2.9845,   3.9529,   4.0366,  -2.5515,  -8.2993,  -3.1049,  -2.6591,  -0.1809,  -3.5850,  -1.8956,   2.6064,  -2.0889,   3.3617,  -2.0492,  -2.6577,   3.7819,  -4.9559,  -6.7673,  -2.7247,  -2.4078,  -0.6825,   3.9704,  10.9501,   2.0463,  -1.4624,   2.5597,   2.0477,  -2.0570,  -3.0077,  -1.4584,   2.2610,   2.7035,   0.8707,   3.8697,   5.5298,   3.9554,   2.2743,   2.4570,  -4.6601,  -5.7113,  -1.8608,  -2.5798,   1.9453,   1.4764,   3.4739,  -2.6324,   2.8100,   4.4935,   5.1273,   1.9984,   0.2825,   1.2956,   2.9882,   2.8723,  -5.1172,  -1.9906,   2.9579,   3.3642,  -1.4127,   2.3386,  -2.4224,   2.9995,  -1.8562,   0.3422,  -0.8372,  -2.4414,   0.8862,  -3.9848,  -4.2464,   1.5061,  -1.7290,  -4.6761,   2.8676,   3.8375,  -7.4214,   0.6649,   3.3717,   2.6966,   3.3336,  -3.3253,   0.3997,  -4.4482,  -3.7917,  -2.7042,   2.2122,   3.6777,   6.7191,   4.1544,   6.7501,   2.1388,  -5.3451,  -1.5311,  -3.4215,   8.9953,   4.8549,  -5.9270,   2.2586,  -2.0082,  -1.6276,  -0.7178,   4.4822,  -4.2229,   5.4284,   4.6790,  -1.6955,   3.4683,   2.4033,  -4.3394,  -5.0685,  -4.4110,   6.2854,   4.6463,   2.1113,  -3.4010,  -1.1522,   2.0507,   3.6134,   2.4586,   5.4234,  -2.9437,   3.5408,  -3.0529,   1.1716,   1.3120,  -1.6787,   1.0602,   4.7967,   1.9569,   4.6120,   5.8092,  -0.9948,  10.5456,  -4.0232,   7.4038,   3.3153,   3.5895,  -1.8777,   1.9847,  -1.7583,  -4.8793,   2.9124,  -3.5227,   2.6751,   2.9501,   3.2102,   2.5796,  -5.1045,  -3.0407,   0.3825,   2.6252,  -2.8446,   2.6536,   3.5893,   4.5472,   2.7247,  -2.5711,   2.4187,   2.9650,   2.9900,   7.6513,  -1.7947,   2.8211,  -2.7685,   1.2563,  -2.1629,   6.0845,  -4.9344,  -8.5401,   2.2989,   1.3754,   0.1441,   0.4399,   2.2272,   3.6265,  -4.4422,  -2.6784,  -4.2967,  -0.5025,  -2.7355,  -0.4416,  -3.7810,   1.5710,  -4.3905,  -3.6649,   1.6868,   1.1499,  -1.6047,   1.0959,   3.3758,  -5.3600,  -3.1644, -11.1107,   0.5539,  -3.1158,   4.4793,   0.5166,  -1.9787,  -4.0968,   3.3299,   3.0844,  -4.6901,   1.5951,   2.4457,   2.5495,   5.2574,  -4.7906,   0.9211,   3.9209,   2.2574,   3.8323,  -1.4021,   2.0486,   0.3764,   0.1444,   6.5957,   2.6252,  -3.4899,  -2.5452,  -2.0719,   2.2205,   4.3362,  -3.4710,   0.7196,   1.6187,   4.9052,  -3.1810,   1.7027,  -2.6991,  -3.3921,   4.0530,  -3.9090,  -5.0446,   2.3771,   3.2310,  -3.5612,  -4.4711,   2.7345,   6.5887,   5.0029, -12.4988,   3.9298,   2.4706,  -1.3161,  -2.4120,  -2.0489,  -2.3337,  -3.8353,   2.4034,  -1.1288,   3.1594,  -4.4244,  -7.8369,   4.7868,  -3.7635,   0.5559,  -4.6655,  -3.8291,  -2.2597,   2.2876,   1.2873,  -2.5493,   1.9032,   5.9153,   4.7153,  -3.7556,  -5.2510,   3.3720,  -2.5286,   1.0339,   3.0323,  -1.5962,  -6.0889,  -0.5129,  -3.8138,   2.8939,   1.9693,  -3.6913,  -4.3571,  11.5029,   3.0059,  -0.4600,  -3.3038,  -2.8480,   3.3966,   4.9615,  -1.3474,  -5.0270,   3.7211,   3.7829,   2.7409,   1.3416,   0.2030,  -2.4366,  -3.5424,  -2.5097,  -3.0251,   5.5744,   3.4625,   5.1421,  -2.5196,  -2.0211,  -2.8836,  -7.9916,   3.2258,  -7.9369,   1.4029,  -4.8988,  -1.9692,   1.5646,   2.3254,  -3.6890,  -3.7811,   4.9147,   4.1513,  -5.2910,   2.5033,   1.5713,  -3.3424,  -7.2111,   2.5959,   5.4492,   2.6996,  -1.8790,  -0.8983,   1.6784,   5.0093,  -0.8171,  -0.6527,  -0.9486,   2.2249,   0.3389,   0.2284,   5.5556,  -2.8920,   4.8645,   0.6409,   0.7171,   3.7462,   6.5847,  -5.6968,   4.2712,   3.2791,   1.8457,  -0.3695,   5.5254,  -0.6961,   1.5406,  -2.1816,  -2.8438,  -8.5958,   2.4107,  -1.4465,   1.3528,  -2.0534,  -4.4886,   5.5551,   2.9101,   9.9892,  -3.2736,  -3.7178,   5.1356,   2.6395,   1.0874,  -3.3337,  -8.8163,   4.6090,  -6.5849, -12.4662,   2.4960,  -3.5745,  -5.0950,   0.3892,   4.9208,  -1.8743,   1.1570,   4.3269,   2.4346,  -0.1776,  -4.2892,  -0.7123,   1.3216,  -3.4085,  -4.5959,   4.8226,   2.8648,  -4.2684,  -2.0437,  -5.9406,  -4.0829,  -2.8765,   3.0194,   8.5074,  -1.0456,   3.1537,   5.1108,  -3.6786,   2.1356,  -1.7903,   3.1888,  -4.7447,   5.9208,   3.2922,   0.9804,  -4.4078,  -2.4620,  -2.4680,   2.3271,  -3.5138,  -4.8362,  -5.5038,  -1.4739,  -1.9759,   3.6498,  -2.9399,  -3.9488,  -1.5633,  -0.1606,  -3.1270,   2.5329,  -0.7261,  -3.1174,  -9.5598,   3.8759,  -7.2873,  -5.4207,  -2.4390,   3.6476,  -2.5205,  -0.8341,   1.9458,   5.6987,  -2.6700,   2.3974,   3.9068,   1.0599,   1.5917,   4.2767,  -3.8219,   4.2708,  -6.4464,   4.6583,  -3.3941,  -5.0541,   4.0098,   2.8200,  -2.6962,  -0.7436,   4.4831,   5.4382,   5.2441,   5.3405,   0.5050,  -5.4305,  -3.7435,  -5.2982,   1.8031,  -4.7520,  -4.4997,   4.9021,  -2.9815,  -4.9378,   3.5109,  -3.1449,  -3.2769,   0.4464,  -4.1655,   2.3491,  -4.5313,   1.7226,   2.0928,  -3.1921,   4.7195,   4.3296,  -2.2977,   2.9951,  -6.4495,   2.8099,  -2.2683,  -2.1251,   2.7506,  -1.4940,  -6.2629, -10.8292,   0.3307,   3.3181,   1.4487,  -0.1487,   3.1136,  -1.0472,  -4.2618,   4.9193,   5.0298,   2.4967,   4.5292,   4.9966,  -1.1003,  -2.4808,  -3.0389,  -4.1720,  -3.0786,   5.7101,  -4.2536,   2.5592,   0.3681,   2.2844,   4.7465,  -2.8661,  -1.0286,   2.6982,   1.5880,  -0.3917,   1.8933,  -5.4936,  10.0773,   2.8386,   0.8830,   5.0484,  -5.4294,  -5.2880,  -0.7906,  -4.0925,  -3.9936,   2.9695,  -0.3174,  -2.0329,  -8.4150,   3.9020,   3.6354,   2.3759,  -3.3053,   0.4248,  -3.3580,  -4.7486,  -5.1739,   3.4083,  -1.5601,   2.9582,  -1.5861,  -2.9597,  -3.0714,   2.0471,  -2.0096,  -5.1898,   4.0593,   1.4882,  -3.0095,   2.1197,   2.1122,  -5.5871,  -5.2619,   3.3380,   4.4519,   4.7074,  -3.2065,   7.4717,   5.1845,  -2.8174,  -3.1582,   4.7520,   3.6389,  -2.8457,  -1.5212,  -6.8247,   0.0525,   5.1890,   3.0828,  -4.9423,  -3.0509,  -1.6793,   5.2776,  -3.8039,  -1.8721,  -5.0509,  -0.0998,   0.5460,  -0.6726,   2.5468,   4.0289,  -1.3560,   5.5364,   0.8601,   1.1279,  -2.1731,  -3.9541,  -3.8548,   0.9774,  -0.1181,  -3.1687,  -4.8524,   0.3642,  -8.0845,   3.5132,   4.6057,  -1.8679,   2.0169,   0.5604,   2.9576,  -3.2010,  -6.0258,   2.0572,   0.8773,  -1.7369,  -3.4186,  -7.3742,  -0.4272,  -1.6428,  -0.6887,  -2.7796,  -2.6484,  -2.0381,   4.1685,   3.7792,   3.0037,   3.3196,  -4.5995,   3.5906,  -6.6125,   3.3731,  -4.4962,  -3.8671,  -0.9857,  -0.5657,   3.1491,   2.3764,   1.9875,  -2.1235,  -2.2715,   4.9033,  -3.1871,  -1.1927,  10.8146,  -0.9809,   3.0042,  -5.8713,  -4.0017,  -2.2456,  -2.9768,  -3.4621,   2.3079,  -2.8899,  -5.1879,  -2.0957,   3.7164,   2.5989,  -1.6385,   2.2381,  -5.2137,   6.2795,   0.3880,   2.2639,  -0.1961,   3.1347,  -9.6741,   4.0620,  -4.7669,   3.5041,   3.2711,  -2.0893,   3.3764,  -3.3009,   9.6466,   3.7193,   3.1521,   0.9499,   0.9278,  -3.1580,   2.7054,   1.5282,   3.4806,  -2.3853,   4.7204,   2.8265,   2.6394,  -6.1245,  -0.3528,  -4.3009,  -3.0127]]], device="mps")

model = HookedTransformer.from_pretrained("gpt2")

while True:
# generate next token
    logits = model(input=first_embedding, return_type="logits", start_at_layer=0)
    next_tok = (torch.topk(logits[0][0], 3).indices[0])


    print(model.tokenizer.convert_ids_to_tokens([next_tok]))

    # get the embedding of the next token
    embedding = model.embed(torch.tensor([[next_tok]]))
    # append this embedding to the first embedding
    first_embedding = torch.cat((first_embedding, embedding), 1)
    print(first_embedding.shape)
