import torch
from transformer_lens import HookedTransformer

first_embedding = torch.tensor([[[ 3.2701e+00,  2.6998e+00,  3.5042e+00,  4.6524e+00, -1.8409e+00,  3.0624e+00,  3.8110e+00,  3.2994e+00,  3.7607e+00,  1.3775e+00,  6.9926e-01, -3.1731e+00,  1.5707e+00,  2.4534e+00, -4.8449e-01,  3.1649e+00, -2.3191e+00,  2.6974e+00,  1.6755e+00,  3.4665e+00,  2.3119e+00, -2.6419e+00, -3.6681e+00,  2.8102e+00,  2.5214e+00,  2.2276e+00,  2.9362e+00,  1.7196e-02, -1.5531e+00, -4.6649e+00, -2.2672e+00, -2.4647e+00,  3.4713e+00, -4.3832e+00,  2.6491e+00,  3.8142e+00,  2.0758e+00,  5.4963e+00,  2.2916e+00, -3.2807e+00, -3.3075e+00,  2.3091e+00, -2.3674e+00,  2.6064e+00, -2.6472e+00, -3.6597e+00, -4.6856e+00, -2.3759e+00,  4.6917e+00, -1.5908e+01,  2.5733e+00, -3.1911e+00, -2.9998e+00,  8.6340e+00, -4.2032e+00, -4.9826e+00,  4.9762e+00,  2.5010e+00, -3.3657e+00,  2.8980e+00, -2.3647e+00, -3.3083e+00, -2.8809e+00,  2.2417e+00, -2.8437e+00, -3.3130e+00, -2.8865e+00,  3.2806e+00,  1.9545e+00,  2.2284e+00, -8.0322e-01, -5.9536e+00, -2.3052e+00,  2.9275e+00,  3.1197e+00,  5.9090e+00, -2.4917e+00, -3.0099e+00,  2.4574e+00,  3.6413e+00, -3.0728e+00, -2.9641e+00, -2.1345e+00, -2.9165e+00,  2.9137e+00, -3.2342e+00, -2.3810e+00, -2.9920e+00, -3.2944e+00, -3.0252e+00, -2.5826e+00,  3.0058e+00,  2.5356e+00,  3.0173e+00,  5.1441e+00,  1.6068e+00,  2.5813e+00, -2.2439e+00,  3.7582e+00, -3.4365e+00,  3.8082e-02,  2.5304e+00, -1.8273e+00,  1.9598e+00, -4.9898e+00, -2.7158e+00,  3.6916e+00, -3.3457e+00, -3.3985e+00,  3.3544e+00, -3.6374e+00, -1.7757e+00,  2.2148e+00, -1.2511e+00,  1.7050e+00, -2.8893e+00,  3.6979e+00,  5.0130e+00,  6.5204e+00,  8.4065e-01, -1.6737e+00,  4.0567e+00,  2.9379e+00,  3.1610e+00,  4.4002e+00,  1.9987e+00,  2.4300e+00, -4.2353e+00,  2.8826e+00, -2.5831e+00, -3.4688e+00,  1.8016e+00, -3.5108e+00,  1.1181e-01,  2.6021e+00, -5.5082e+00, -2.6936e+00,  2.7971e+00,  2.6515e+00,  1.9828e+00, -7.7539e-01,  1.1509e+00,  3.2720e+00, -1.3907e+00,  2.1558e+00,  4.1376e+00, -3.3117e+00,  2.4272e+00,  7.4710e-01, -3.1716e+00,  4.1158e-01, -2.8483e+00, -2.6375e+00, -2.5479e+00, -2.0741e+00,  3.1177e+00, -7.6702e+00,  2.5939e+00,  3.1738e-01, -1.6683e+00,  4.1985e+00, -3.1051e+00, -2.0540e+00,  3.5996e+00,  2.1582e+00, -1.1076e+00, -3.0139e+00,  1.0741e+01,  2.2842e+00,  6.0744e+00, -2.9744e+00, -1.9497e+00,  3.3308e+00, -6.1160e+00, -8.4701e-01,  2.3406e+00,  3.3827e+00,  2.9449e+00, -3.3055e+00,  2.6924e+00,  3.2107e+00, -2.7593e+00,  3.7150e+00, -3.2523e+00,  4.6710e-01,  2.8375e+00, -7.4071e+00, -3.4078e+00,  1.4412e-01, -4.8602e+00, -3.4179e+00,  6.5941e+00, -2.6523e+00, -2.6112e+00, -2.6156e-01, -4.0520e+00,  1.6358e+00,  2.7021e+00, -2.3878e+00, -7.0027e-01, -1.0996e+00,  1.8594e+00,  3.1466e+00, -1.2931e+00, -2.3441e+00, -6.7154e-01,  4.0165e+00,  3.1372e+00,  4.2659e+00,  1.3408e+01, -4.6919e+00, -2.5871e+00, -4.1092e+00,  2.7809e+00,  3.9446e+00, -2.7536e+00, -2.7273e+00,  1.8160e+00,  3.2824e+00,  3.4049e+00, -6.4164e-01,  3.0702e+00, -9.4726e-01,  1.5017e+00, -1.3362e+00,  1.3217e+01,  2.6815e+00, -2.9605e+00,  3.0687e+00,  2.3342e+00, -8.3511e-01,  2.5646e+00, -7.7142e+00,  3.7766e+00,  3.4504e+00,  3.4031e+00,  3.8746e+00, -2.6233e+00, -3.1209e+00,  3.2807e+00,  3.8220e+00, -2.7892e+00, -6.4008e-02,  2.9253e-01, -1.8093e+00,  2.2187e+00, -4.5709e+00,  2.9449e+00,  2.9880e+00, -3.7173e+00,  3.5564e+00, -2.8638e+00,  5.9358e+00, -2.0329e+00,  2.6870e+00, -3.4222e+00,  3.9685e+00, -2.2893e-01, -5.0519e+00,  2.8921e+00,  5.7653e+00,  4.1852e+00, -8.8981e+00,  1.4145e+00,  3.4296e+00,  1.9227e+00, -2.8613e+00, -3.7629e+00, -4.0419e+00, -3.2625e+00, -2.9103e+00, -2.8441e+00,  2.6733e+00, -3.8046e+00,  3.5798e+00,  1.8640e+00, -8.7379e-01, -1.3210e+00,  2.4500e+00,  2.9530e+00,  2.3253e+00, -3.0252e+00, -4.5410e+00, -3.0713e+00, -3.9290e+00,  3.0886e+00,  1.5457e+00,  9.0068e-01, -3.3333e+00,  3.6262e+00,  2.8358e+00, -3.0424e+00, -1.9285e+00, -3.8731e+00,  1.8229e+00, -2.7704e+00, -3.3604e+00, -2.8749e+00,  3.2278e+00,  5.4143e+00,  2.0989e+00, -6.4621e+00,  2.5345e+00,  2.1428e+00,  2.4839e+00,  4.3190e+00,  3.5073e+00, -3.3844e+00, -1.1774e+00,  4.0487e+00,  3.2621e+00,  2.9512e+00, -3.2026e+00,  2.7320e+00,  6.3544e+00, -4.6095e+00, -3.2789e+00,  7.4386e-01, -3.2586e+00, -3.2986e+00,  4.1284e+00,  3.6699e+00,  3.6028e+00,  3.0189e+00,  5.1047e+00,  1.7755e+00, -3.5197e+00,  3.2981e+00, -2.8755e+00,  3.0532e+00, -2.2675e+00, -4.8281e+00,  1.7305e+00,  1.4851e-01, -3.1826e+00,  1.9847e+00, -3.1973e+00, -3.2897e+00,  2.2712e+00, -3.9274e+00,  1.1055e+00,  2.1331e+00, -2.6468e+00, -1.0134e+01, -3.3693e+00,  2.7112e+00, -1.6290e+00, -1.2645e+00, -2.5903e+00,  9.5423e-01, -4.2640e+00, -2.3756e+00,  2.0289e+00,  2.6377e+00, -2.3528e-01, -2.5957e+00, -2.8448e+00,  4.2405e+00, -1.8731e+00, -2.7920e+00,  7.2146e+00, -2.7831e+00,  1.3939e-01, -2.9906e+00, -2.6641e+00,  3.8480e+00, -1.9745e+00, -2.3207e+00,  3.2070e+00,  2.4973e+00,  5.2619e+00, -4.2035e+00, -3.8441e-01, -2.8699e+00,  3.9876e+00,  2.7011e+00, -1.0498e+01,  3.1264e+00,  3.0842e+00,  3.1707e+00,  1.3402e-01,  2.3724e+00, -1.0948e+00, -3.2998e+00, -2.8531e+00, -2.8825e+00, -4.5533e+00,  2.1974e+00,  1.1177e+00,  5.4661e+00, -4.4817e+00, -2.5041e+00, -3.5851e+00, -1.4586e+00,  3.1943e+00, -9.4492e-01,  3.6605e+00, -2.2560e+00, -2.6033e-01, -2.1963e+00,  3.1102e+00,  3.5540e+00, -3.9181e+00, -1.3025e+00,  2.7014e+00, -2.3678e+00,  2.2280e+00,  3.3339e+00, -4.6186e+00, -8.0194e-01, -2.7510e+00,  3.0240e+00,  3.0976e+00, -2.4658e+00,  1.0434e+00, -3.6575e+00, -3.3309e+00, -1.6018e+00,  1.5958e+01, -3.9625e+00, -5.4506e+00,  2.8158e+00,  3.1458e+00,  4.2186e+00, -3.0273e+00,  3.5079e+00,  1.6179e+00, -3.7593e+00,  5.8660e-01, -1.8727e+00, -2.0763e+00,  3.9866e+00,  2.4571e+00,  3.7268e+00,  1.0837e+00, -3.8219e+00, -3.7114e+00, -3.9907e+00, -4.0609e+00, -3.3135e+00, -2.8304e+00, -1.3136e+00, -1.7067e+00,  1.6541e+00, -2.7152e+00,  2.8485e+00,  4.4686e+00,  2.8559e+00,  4.6016e+00, -5.1164e+00, -6.4938e+00,  3.9908e+00,  3.6644e+00, -3.9087e+00, -2.5883e+00, -5.6636e+00,  3.6391e+00, -3.9557e+00,  2.4983e+00,  2.3427e+00, -2.4562e+00, -3.1857e+00, -5.9040e+00,  2.0192e+00, -2.1992e+00,  2.3433e+00,  3.8737e+00,  1.2109e+00,  1.8867e+00,  3.4577e+00,  3.3313e+00,  3.6686e-01, -2.2835e+00, -3.8217e+00,  5.9930e+00, -2.7243e+00, -3.0712e+00,  1.5261e+00, -1.7718e+00,  1.5744e+00, -2.9591e+00,  2.9248e+00,  1.8708e+00,  5.3615e+00,  3.3852e+00,  2.6692e+00,  4.1787e+00, -1.6217e-01, -3.0029e+00,  9.2783e-01, -1.5617e+00,  6.9376e+00, -3.8004e+00, -2.4029e+00, -2.1553e+00, -3.8041e+00, -3.8257e+00,  2.4929e+00, -2.5609e+00, -2.2913e+00,  4.6362e+00,  8.9068e-01, -3.8689e+00,  3.0518e+00, -2.7229e+00, -3.7255e+00, -2.6981e+00,  1.4267e+01, -3.4722e+00,  2.2699e-01, -1.1086e+00, -2.9648e+00, -5.5616e+00,  2.6240e+00, -2.6386e+00,  2.7462e+00, -2.8874e+00,  4.3028e+00, -2.2486e+00, -2.3625e+00,  1.5527e+00,  2.6027e+00, -2.9834e+00,  2.2763e+00,  3.9736e+00,  9.7281e-01, -7.8785e-01,  3.2398e+00, -3.8568e+00,  2.1621e+00, -3.6172e+00,  1.6278e+00, -3.0316e+00, -3.7398e+00,  2.7140e+00, -1.4660e+00, -2.6267e+00,  2.6120e+00,  4.0151e+00,  6.7557e+00,  1.5626e+00,  4.3509e-02,  2.2411e+00, -1.5967e+01, -2.8238e+00, -8.5374e+00,  3.0397e+00, -3.3131e+00,  8.7444e+00,  2.3615e+00,  2.6961e+00,  2.0734e+00, -4.8381e+00, -4.1895e+00, -1.8853e+00, -3.8705e+00, -5.9332e-01, -3.0480e+00,  2.2790e+00,  3.0182e+00,  2.8634e+00, -2.5753e+00, -8.7511e-01,  4.1064e+00, -3.3461e+00,  2.7565e+00, -1.4409e+01,  4.4283e+00, -2.3059e+00, -3.2162e+00,  2.9622e+00,  2.9744e+00, -4.0754e+00, -3.1994e+00,  4.9481e-01,  3.8378e+00,  3.4188e+00,  2.8171e+00,  1.9978e+00, -2.3645e+00, -4.2683e+00, -8.3694e-01, -5.3123e+00,  4.0307e+00, -5.4208e+00,  3.1464e+00, -1.9149e+00, -2.9051e+00, -4.1305e+00, -4.8588e+00, -2.2716e+00,  2.3143e+00, -4.0652e+00,  2.7846e+00, -3.3178e+00, -2.6666e+00,  2.5130e+00, -1.1796e+00,  1.4946e+00,  6.4137e+00,  6.3675e+00,  1.7157e+00,  3.2739e+00, -3.0705e+00,  2.8974e+00,  1.6235e+00,  1.0192e+00,  2.4067e+00, -3.1676e+00, -3.7936e+00, -2.7702e+00, -5.8816e-01,  1.3371e+01,  2.2953e+00,  2.8718e+00, -2.6200e+00, -6.5383e+00,  2.7985e+00,  2.3878e+00,  3.5756e+00, -3.0335e+00,  2.8729e+00, -2.3195e+00, -3.2371e+00,  2.6632e+00, -6.2558e+00, -1.4277e+00,  3.0755e+00, -3.7881e+00, -2.3527e+00,  5.9502e+00,  3.2174e+00, -2.6847e+00, -4.7403e-01,  2.0246e+00,  7.4279e-01,  1.5957e-02,  2.1760e+00,  4.7410e+00,  1.3240e+00, -2.6960e+00, -8.1189e-01, -4.6102e+00, -1.3750e+00,  3.3769e+00,  3.7262e+00,  3.8540e+00, -2.8310e+00, -3.2895e+00,  2.6924e+00, -1.9196e+00,  5.3797e-01, -2.8106e+00, -2.6616e+00,  1.9992e+00,  3.2205e+00,  2.4876e+00, -2.6292e+00, -1.9065e+00, -2.4534e+00,  2.2020e+00, -3.8469e+00, -2.8091e+00, -2.0101e+00,  2.7148e+00, -5.4906e+00,  2.4567e+00,  3.7499e+00, -6.0192e+00, -2.2679e+00,  1.0046e+00,  6.4927e+00,  2.9126e+00, -3.1498e+00, -9.3713e-01,  3.1810e+00, -3.6346e+00,  4.0271e+00, -3.6674e+00, -4.3824e+00,  1.2766e+00,  3.4932e+00,  2.4471e+00,  2.8733e+00, -5.8671e+00,  3.1199e+00, -1.1113e+01,  2.7188e+00, -4.0044e+00,  2.0849e+00,  2.9715e+00,  2.5856e+00,  1.8697e+00, -5.1456e+00, -1.7416e+00, -2.8541e+00, -1.8083e+00, -3.4837e+00, -6.1303e+00, -1.7987e+00, -3.3173e+00,  2.7708e+00,  3.7963e+00,  4.6467e-01,  2.7193e+00, -3.1424e+00,  4.3371e+00,  3.4736e+00,  3.9058e+00,  2.6985e+00,  4.9804e+00, -1.2884e+00,  7.4179e-01, -2.5228e+00, -2.8646e+00,  6.5392e+00, -5.0302e-01, -3.4004e+00,  3.7900e+00,  1.0456e+00, -2.2111e+00, -2.3647e+01, -3.4073e+00,  9.2240e-01, -2.8280e+00, -3.3096e+00, -2.6256e+00, -2.7756e+00, -3.9597e+00,  3.1076e+00, -3.0918e+00,  5.0734e+00, -1.8057e+00, -8.5104e-01, -1.7544e+00,  3.6696e+00,  3.0666e+00,  3.1658e+00,  3.8232e-02,  4.7163e+00,  2.5646e+00,  2.8233e+00,  3.4009e+00, -3.9972e+00,  1.8650e+00, -6.8363e-01,  3.0581e+00,  2.7519e+00, -1.6348e+00,  3.3245e+00, -2.6158e+00, -2.6514e-01,  5.1016e+00, -1.9967e+00,  3.6408e+00,  2.8903e+00,  4.8037e+00,  2.6480e+00, -2.6158e+00,  3.0277e+00, -3.0827e+00,  1.9541e+00, -8.3992e+00,  3.1828e+00, -2.4597e+00,  2.0554e+00, -2.9378e+00,  6.4775e-01]]], device="mps")

model = HookedTransformer.from_pretrained("gpt2")

while True:
# generate next token
    logits = model(input=first_embedding, return_type="logits", start_at_layer=0)
    # randomly sample from indice
    next_tok = (torch.topk(logits[0][0], 3).indices[0])


    print(model.tokenizer.convert_ids_to_tokens([next_tok]))

    # get the embedding of the next token
    embedding = model.embed(torch.tensor([[next_tok]]))
    print(embedding.shape)
    # append this embedding to the first embedding
    first_embedding = torch.cat((first_embedding, embedding), 1)
    print(first_embedding.shape)
