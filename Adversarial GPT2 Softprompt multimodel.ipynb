{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIr3ZNmedIg4",
    "outputId": "09af3389-9c9b-48f7-bcb1-cffff05f47a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-0.5b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## Updated by gavento based on code by Sudarsh\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model1 = HookedTransformer.from_pretrained(\"gpt2\", device=device)\n",
    "model2 = HookedTransformer.from_pretrained(\"Qwen/Qwen2-0.5b\")\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PR7dgCGc9_u",
    "outputId": "1f665d4e-1728-4829-bd1b-c6103f63748c"
   },
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "input_text = \"\"\"Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference; and practical, such as applied statistics, fluid mechanics, neuroscience, bioinformatics, and machine learning.\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "model1tokens = model1.to_tokens(input_text)[:, 1:] # Skip BOS\n",
    "model2tokens = model2.to_tokens(input_text)[:, 1:]\n",
    "\n",
    "\n",
    "# embeddings_tree = BallTree(W_E.numpy(), leaf_size=1)\n",
    "\n",
    "SOFT_TOKENS = 10\n",
    "MODEL_1_DIM = 768\n",
    "MODEL_2_DIM = 896\n",
    "L2REG = 0.5\n",
    "\n",
    "def exec_model(model, first_tokens_embedding, tokens):\n",
    "    residual, _tks, _spe, _attn = model.input_to_embed(tokens)\n",
    "    both = torch.concat([first_tokens_embedding, residual], axis=1)\n",
    "    #print(first_tokens_embedding.shape, residual.shape, both.shape)\n",
    "    return model(both, start_at_layer=0)\n",
    "\n",
    "def predict(model, first_tokens_embedding, num_tokens):\n",
    "    tokens = []\n",
    "    for i in range(num_tokens):\n",
    "        toks = torch.tensor(tokens, dtype=torch.long)\n",
    "        residual, _tks, _spe, _attn = model.input_to_embed(toks)\n",
    "        both = torch.concat([first_tokens_embedding.detach(), residual], axis=1)\n",
    "        res = model(both, start_at_layer=0)[0]\n",
    "        next_token = torch.argmax(res, axis=-1)[-1].item()  # Get the last token predicted\n",
    "        tokens.append(next_token)\n",
    "        #print(model.tokenizer.convert_ids_to_tokens(tokens))\n",
    "        #print(residual.shape, toks.shape, both.shape, res.shape, probs.shape)\n",
    "    \n",
    "    return (model.tokenizer.decode(tokens),\n",
    "            model.tokenizer.convert_ids_to_tokens(tokens),\n",
    "            torch.tensor(tokens).cpu())  # Convert the final token list to a tensor\n",
    "\n",
    "\n",
    "def token_alignment_loss(logits, tokens, first_tokens_embedding, gamma=0.2):\n",
    "    # Calculate the cross-entropy loss\n",
    "    def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "    logits = logits[SOFT_TOKENS - 1:-1, :]\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, tokens)\n",
    "    l2_loss = l2(first_tokens_embedding)\n",
    "\n",
    "    total_loss = ce_loss + gamma * l2_loss\n",
    "\n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cIo3gGq3An",
    "outputId": "3adf9e89-092c-46a6-8437-08a6a6929c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Step 0, Maximum Correct=0, Correct=0, Loss=567.2493896484375/0, 24.535686492919922, 542.7136840820312 L2=80.63400268554688, LR=[0.19894313879210052], Pred=' the the.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', Pred2='自营的“ 遥 远 远 远 远 远 远 远'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, Maximum Correct=0, Correct=0, Loss=321.9294128417969/0, 18.26626968383789, 303.66314697265625 L2=60.83761978149414, LR=[0.15264320137581455], Pred='.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', Pred2='Relative entropy is always a non-negative integer, zero, or a finite real number, real, rational'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "Step 100, Maximum Correct=1, Correct=1, Loss=178.6014404296875/1, 15.949251174926758, 162.65219116210938 L2=52.91704559326172, LR=[0.1171186252902463], Pred='L the the the the the the the the the the the the the the the the the the the', Pred2='Relative entropy is a function of the entropy of the system, which is a function of the system’s'\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "\n",
      "Step 150, Maximum Correct=1, Correct=0, Loss=103.55522155761719/1, 13.780649185180664, 89.77457427978516 L2=40.08237075805664, LR=[0.08986166606992071], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions'\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "\n",
      "Step 200, Maximum Correct=3, Correct=3, Loss=66.71804809570312/2, 10.089561462402344, 56.62848663330078 L2=30.7872257232666, LR=[0.06894820536742106], Pred='Relative to the the the the the the the the the the the the the the the the the the', Pred2=' 、16 、24 、32 、48 、64 '\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "\n",
      "Step 250, Maximum Correct=3, Correct=1, Loss=63.12352752685547/2, 11.962411880493164, 51.16111755371094 L2=25.1990966796875, LR=[0.05290192393816882], Pred=') in the the the the the the the the the the the the the the the the the the the the', Pred2=', and the definition of a group is that it is closed under the group operation, i.e., if $'\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "\n",
      "Step 300, Maximum Correct=3, Correct=0, Loss=39.993736267089844/2, 8.204689025878906, 31.789047241210938 L2=21.295719146728516, LR=[0.040590085578676736], Pred='-\\n\\n\\n\\nA-\\nA\\nA\\nA\\nA\\nB\\nB\\nB\\n', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in'\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "\n",
      "Step 350, Maximum Correct=5, Correct=5, Loss=33.085533142089844/3, 5.993043899536133, 27.092491149902344 L2=17.856151580810547, LR=[0.031143575216847408], Pred='Relative entropy is the non-negative number).\\n\\n\\nTheory of the number of the number of', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in'\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "\n",
      "Step 400, Maximum Correct=5, Correct=1, Loss=38.02002716064453/3, 6.549062728881836, 31.470964431762695 L2=16.24455451965332, LR=[0.023895546497614755], Pred='Rel, and not to the left, or to the left, and the right of the right of the left-', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question'\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "\n",
      "Step 450, Maximum Correct=5, Correct=2, Loss=41.307830810546875/3, 8.047033309936523, 33.26079559326172 L2=15.185112953186035, LR=[0.018334347885363634], Pred='le thermals of in the other-\\n of the the the the the the the the the the the the the', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question'\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "\n",
      "Step 500, Maximum Correct=5, Correct=4, Loss=23.296234130859375/4, 4.349095821380615, 18.9471378326416 L2=13.439353942871094, LR=[0.014067404251042834], Pred='Relative entropy is a positive or is a negative of the value of the value of the value of the value of', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question'\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "\n",
      "Step 550, Maximum Correct=13, Correct=13, Loss=19.173479080200195/7, 3.527559757232666, 15.645919799804688 L2=12.318570137023926, LR=[0.010793504279486036], Pred='Relative entropy is always a non-negative real number, with no additional information is no more or less than some other', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are'\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "\n",
      "Step 600, Maximum Correct=14, Correct=14, Loss=18.581501007080078/9, 2.653036594390869, 15.928464889526367 L2=11.422527313232422, LR=[0.008281537414597796], Pred='Relative entropy is always a non-negative real number, with value 1 of of of of of of of of of of of of', Pred2='Relative entropy is defined as $H(X,Y) = \\\\sum_{i=1}^n \\\\log \\\\frac{p_i'\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "\n",
      "Step 650, Maximum Correct=21, Correct=21, Loss=15.525823593139648/12, 2.798180103302002, 12.727643013000488 L2=10.882156372070312, LR=[0.00635417934467609], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two components are identical. The two components of', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse'\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "\n",
      "Step 700, Maximum Correct=21, Correct=21, Loss=16.067378997802734/15, 2.7745578289031982, 13.292821884155273 L2=10.36413860321045, LR=[0.004875374356594533], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two components of the two components are different from left-body', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both'\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "\n",
      "Step 750, Maximum Correct=22, Correct=22, Loss=16.220396041870117/17, 2.5901131629943848, 13.630282402038574 L2=10.026934623718262, LR=[0.0037407309154493533], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two sides of the two sides in a side.\\nTheoretically,', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such'\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "\n",
      "Step 800, Maximum Correct=37, Correct=37, Loss=12.364913940429688/22, 2.0414345264434814, 10.323479652404785 L2=9.658337593078613, LR=[0.0028701524761624154], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as character'\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "\n",
      "Step 850, Maximum Correct=41, Correct=41, Loss=12.700458526611328/27, 2.1716415882110596, 10.528817176818848 L2=9.459807395935059, LR=[0.0022021833226225766], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative stability', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Sh'\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "\n",
      "Step 900, Maximum Correct=41, Correct=41, Loss=12.085346221923828/31, 2.2586910724639893, 9.826655387878418 L2=9.266497611999512, LR=[0.0016896702968621593], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative stability of interest to bef', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information'\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "\n",
      "Step 950, Maximum Correct=41, Correct=41, Loss=12.678628921508789/34, 2.161320209503174, 10.517308235168457 L2=9.157154083251953, LR=[0.0012964341718373653], Pred='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative number of them all sorts of them all sorts of', Pred2='Relative entropy is always a non-negative real number, with value 0 if and only if the two distributions in question are identical. It has diverse applications, both theoretical, such as characterizing the relative (Shannon) entropy in information systems, randomness in'\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7edc3028e5b0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "first_tokens_embedding = torch.randn(size=(1, SOFT_TOKENS, MODEL_1_DIM), dtype=torch.float32, device=device)\n",
    "translation_matrix = torch.randn(size=(MODEL_1_DIM, MODEL_2_DIM)).cuda()\n",
    "if device == \"cuda\": \n",
    "    first_tokens_embedding = first_tokens_embedding.cuda()\n",
    "if device == \"mps\":\n",
    "    first_tokens_embedding = first_tokens_embedding.to(torch.float32).to(device)\n",
    "\n",
    "first_tokens_embedding = first_tokens_embedding.requires_grad_(True)\n",
    "\n",
    "num_steps = 1000  # Number of optimization steps\n",
    "optimizer = torch.optim.Adam([first_tokens_embedding, translation_matrix], lr=0.02 * SOFT_TOKENS, amsgrad=True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "last_corr = 0\n",
    "max_corr = 0\n",
    "lossahead = 5 + SOFT_TOKENS * 2\n",
    "lookahead = 20\n",
    "kappa = 0.1\n",
    "alpha = 0.0\n",
    "beta = 1.0\n",
    "for step in range(num_steps):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_1= exec_model(model1, first_tokens_embedding, model1tokens)\n",
    "\n",
    "    flattened_logits_1 = logits_1.flatten(0, 1)[:last_corr+lookahead+SOFT_TOKENS]\n",
    "    flattened_tokens_1 = model1tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    logits_2 = exec_model(model2, \n",
    "                          first_tokens_embedding @ translation_matrix, \n",
    "                          model2tokens)\n",
    "\n",
    "    flattened_logits_2 = logits_2.flatten(0, 1)[:last_corr + lookahead + SOFT_TOKENS]\n",
    "    flattened_tokens_2 = model2tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    loss_1 = token_alignment_loss(flattened_logits_1, flattened_tokens_1, first_tokens_embedding)\n",
    "    loss_2 = token_alignment_loss(flattened_logits_2, flattened_tokens_2,  first_tokens_embedding @ translation_matrix)\n",
    "\n",
    "    loss = loss_1 + loss_2\n",
    "    \n",
    "    # Backpropagate the loss with retain_graph=True\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the first token's embedding\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    # Print the loss for every 100 steps\n",
    "    print(step)\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():  # Disable gradient computation for prediction\n",
    "\n",
    "            ps = predict(model1, first_tokens_embedding, last_corr+lookahead)\n",
    "            ps_2 = predict(model2,  first_tokens_embedding @ translation_matrix, last_corr + lookahead)\n",
    "            temp_corr = (ps[2][:min(last_corr+lookahead, model1tokens.shape[1])] == model1tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "            max_corr = max(temp_corr, max_corr)\n",
    "            if temp_corr > last_corr:\n",
    "                last_corr += math.ceil((temp_corr - last_corr) / 4)\n",
    "            print(f\"\\nStep {step}, Maximum Correct={max_corr}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, {loss_1.item()}, {loss_2.item()} L2={l2(first_tokens_embedding).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}, Pred2={ps_2[0]!r}\")\n",
    "    \n",
    "\n",
    "    # just comment out this part if you want to not add tokens\n",
    "    # if step % 20 == 0 and step != 0:\n",
    "    #     new_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, 1, MODEL_DIM)), dtype=torch.float32, requires_grad=True).to(device)\n",
    "    #     print(\"ADDING NEW TOK\")\n",
    "    #     first_tokens_embedding = torch.cat([ new_tokens_embedding, first_tokens_embedding], dim=1)\n",
    "    #     first_tokens_embedding = first_tokens_embedding.detach().requires_grad_(True)\n",
    "\n",
    "    #     # Reinitialize the optimizer with the updated embedding\n",
    "    #     optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauI6iQcA3x5",
    "outputId": "f5904061-a6d0-471f-bbd2-a23a818aea05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74]) tensor([[ 9444,  7314,   318, 14394,  1871, 21262,   351,  2754,   284,   644,   262,  4036,  2995,  7346,   428,  1785,    13,   383, 33578,   286,  7900,  2797,  2304,   290,   569,  8207,   355, 28009,    82,   287,  2968,  3968,   318, 29811,   284,   416,   883,  4837,   508,  4968,   262,  9495,   326,   484,   547, 12864,    13,   383,  9450,  5369,   290,   262,  4036, 32896,  5139,  1438,   286,   262, 34692,  6412,   284,   355,  7900, 34115,   291,   389,   635,   407, 26208,  4987,  2402,   416,  7035,    13]], device='cuda:0')\n",
      "['Cons', 'ensus', 'Ġis', 'Ġlacking', 'Ġamong', 'Ġhistorians', 'Ġwith', 'Ġregard', 'Ġto', 'Ġwhat', 'Ġthe', 'Ġactual', 'Ġevents', 'Ġsurrounding', 'Ġthis', 'Ġevent', '.', 'ĠThe', 'Ġportrayal', 'Ġof', 'ĠAh', 'unt', 'isc', 'Ġand', 'ĠV', 'iel', 'Ġas', 'Ġmartyr', 's', 'Ġin', 'Ġpopular', 'Ġculture', 'Ġis', 'Ġobjected', 'Ġto', 'Ġby', 'Ġthose', 'Ġresearchers', 'Ġwho', 'Ġreject', 'Ġthe', 'Ġnotion', 'Ġthat', 'Ġthey', 'Ġwere', 'Ġmurdered', '.', 'ĠThe', 'Ġethnic', 'Ġidentity', 'Ġand', 'Ġthe', 'Ġactual', 'Ġphon', 'etic', 'Ġname', 'Ġof', 'Ġthe', 'Ġmissionary', 'Ġreferred', 'Ġto', 'Ġas', 'ĠAh', 'unts', 'ic', 'Ġare', 'Ġalso', 'Ġnot', 'Ġuniversally', 'Ġagreed', 'Ġupon', 'Ġby', 'Ġauthors', '.']\n",
      "0\n",
      "\n",
      "Step 0, Maximum Correct=0, Correct=0, Loss=8.309679985046387/0, L2=88.7891616821289, LR=[0.19894313879210052], Pred='ers, and T, and T, and T, and T, and T, T, T'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, Maximum Correct=20, Correct=20, Loss=1.82130765914917/5, L2=154.36643981933594, LR=[0.15264320137581455], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "Step 100, Maximum Correct=25, Correct=25, Loss=1.8166347742080688/10, L2=170.41490173339844, LR=[0.1171186252902463], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and V'\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "\n",
      "Step 150, Maximum Correct=30, Correct=30, Loss=1.8154685497283936/15, L2=177.44131469726562, LR=[0.08986166606992071], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in'\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "\n",
      "Step 200, Maximum Correct=35, Correct=35, Loss=1.817932367324829/20, L2=184.32774353027344, LR=[0.06894820536742106], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to'\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "\n",
      "Step 250, Maximum Correct=40, Correct=40, Loss=1.8215197324752808/25, L2=189.73992919921875, LR=[0.05290192393816882], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject'\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "\n",
      "Step 300, Maximum Correct=45, Correct=45, Loss=1.8197252750396729/30, L2=191.70199584960938, LR=[0.040590085578676736], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were'\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "\n",
      "Step 350, Maximum Correct=50, Correct=50, Loss=1.8259745836257935/35, L2=193.94073486328125, LR=[0.031143575216847408], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity'\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "\n",
      "Step 400, Maximum Correct=55, Correct=55, Loss=1.8314530849456787/40, L2=194.82492065429688, LR=[0.023895546497614755], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic'\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "\n",
      "Step 450, Maximum Correct=60, Correct=60, Loss=1.8364484310150146/45, L2=196.18287658691406, LR=[0.018334347885363634], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred'\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "\n",
      "Step 500, Maximum Correct=65, Correct=65, Loss=1.838527798652649/50, L2=196.99415588378906, LR=[0.014067404251042834], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic'\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "\n",
      "Step 550, Maximum Correct=65, Correct=65, Loss=1.8864409923553467/54, L2=197.43431091308594, LR=[0.010793504279486036], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic is not universally agreed upon'\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "\n",
      "Step 600, Maximum Correct=72, Correct=72, Loss=1.8997751474380493/59, L2=197.7508087158203, LR=[0.008281537414597796], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by author the'\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "\n",
      "Step 650, Maximum Correct=74, Correct=74, Loss=1.8425408601760864/63, L2=197.8188934326172, LR=[0.00635417934467609], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is not universally'\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "\n",
      "Step 700, Maximum Correct=74, Correct=74, Loss=1.834802508354187/66, L2=197.83120727539062, LR=[0.004875374356594533], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The'\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "\n",
      "Step 750, Maximum Correct=74, Correct=74, Loss=1.8312660455703735/68, L2=197.83718872070312, LR=[0.0037407309154493533], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The author by the'\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "\n",
      "Step 800, Maximum Correct=74, Correct=74, Loss=1.8291947841644287/70, L2=197.84071350097656, LR=[0.0028701524761624154], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The author by the official by'\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "\n",
      "Step 850, Maximum Correct=74, Correct=74, Loss=1.8278559446334839/71, L2=197.84303283691406, LR=[0.0022021833226225766], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The author by the official by the official'\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "\n",
      "Step 900, Maximum Correct=74, Correct=74, Loss=1.826940655708313/72, L2=197.84466552734375, LR=[0.0016896702968621593], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The author by the official by the official by'\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "\n",
      "Step 950, Maximum Correct=74, Correct=74, Loss=1.8262921571731567/73, L2=197.84584045410156, LR=[0.0012964341718373653], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors. The media is widely held by authors. The author by the official by the official by the'\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "text_two = \"Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors.\"\n",
    "\n",
    "first_embed = torch.randn(size=(1, SOFT_TOKENS, MODEL_1_DIM), dtype=torch.float32, device=device, requires_grad=True).cuda()\n",
    "\n",
    "\n",
    "model = model1\n",
    "tokens = model.to_tokens(text_two)[:, 1:] # Skip BOS\n",
    "print(tokens.shape, tokens)\n",
    "print(model.tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.Adam([first_embed], lr=0.02 * SOFT_TOKENS, amsgrad=True)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "last_corr = 0\n",
    "max_corr = 0\n",
    "lossahead = 5 + SOFT_TOKENS * 2\n",
    "lookahead = 20\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Assuming exec_model correctly processes the model input with first_tokens_embedding\n",
    "    logits = exec_model(model, first_embed, tokens)\n",
    "\n",
    "    flattened_logits = logits.flatten(0, 1)[:last_corr+lookahead+SOFT_TOKENS]\n",
    "    flattened_tokens = tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    loss = token_alignment_loss(flattened_logits, flattened_tokens, first_tokens_embedding)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    print(step)\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():  # Disable gradient computation for prediction\n",
    "            ps = predict(model, first_embed, last_corr+lookahead)\n",
    "            temp_corr = (ps[2][:min(last_corr+lookahead, tokens.shape[1])] == tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "            max_corr = max(temp_corr, max_corr)\n",
    "            if temp_corr > last_corr:\n",
    "                last_corr += math.ceil((temp_corr - last_corr) / 4)\n",
    "            print(f\"\\nStep {step}, Maximum Correct={max_corr}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, L2={l2(first_embed).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
