{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIr3ZNmedIg4",
    "outputId": "09af3389-9c9b-48f7-bcb1-cffff05f47a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-0.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## Updated by gavento based on code by Sudarsh\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2-0.5B\", device=device)\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pynndescent import NNDescent\n",
    "\n",
    "# W_E = model.embed._parameters[\"W_E\"].detach().cpu()\n",
    "# index = NNDescent(W_E)\n",
    "# index.prepare()\n",
    "\n",
    "import faiss\n",
    "\n",
    "resources = faiss.StandardGpuResources()\n",
    "index = faiss.IndexFlatL2(model.embed._parameters[\"W_E\"].shape[1])\n",
    "index = faiss.index_cpu_to_gpu(resources, 0, index)\n",
    "\n",
    "W_E = model.embed._parameters[\"W_E\"].detach()\n",
    "\n",
    "index.add(model.embed._parameters[\"W_E\"].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PR7dgCGc9_u",
    "outputId": "1f665d4e-1728-4829-bd1b-c6103f63748c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74]) tensor([[15220, 13626,   374, 31061,  4221, 50701,   448,  5250,   311,  1128,   279,  5042,  4357, 14590,   419,  1538,    13,   576, 73933,   315, 16366,  3850,  3427,   323, 11401,   301,   438, 59349,  5428,   304,  5411,  7674,   374, 75779,   311,   553,  1846, 11811,   879,  7850,   279, 22240,   429,   807,  1033, 31385,    13,   576, 21551,  9569,   323,   279,  5042, 50823,  5298,   829,   315,   279, 72898, 13862,   311,   438, 16366, 36940,   292,   525,  1083,   537, 60428,  7230,  5193,   553, 12014,    13]], device='cuda:0')\n",
      "['Cons', 'ensus', 'Ġis', 'Ġlacking', 'Ġamong', 'Ġhistorians', 'Ġwith', 'Ġregard', 'Ġto', 'Ġwhat', 'Ġthe', 'Ġactual', 'Ġevents', 'Ġsurrounding', 'Ġthis', 'Ġevent', '.', 'ĠThe', 'Ġportrayal', 'Ġof', 'ĠAh', 'unt', 'isc', 'Ġand', 'ĠVi', 'el', 'Ġas', 'Ġmarty', 'rs', 'Ġin', 'Ġpopular', 'Ġculture', 'Ġis', 'Ġobjected', 'Ġto', 'Ġby', 'Ġthose', 'Ġresearchers', 'Ġwho', 'Ġreject', 'Ġthe', 'Ġnotion', 'Ġthat', 'Ġthey', 'Ġwere', 'Ġmurdered', '.', 'ĠThe', 'Ġethnic', 'Ġidentity', 'Ġand', 'Ġthe', 'Ġactual', 'Ġphon', 'etic', 'Ġname', 'Ġof', 'Ġthe', 'Ġmissionary', 'Ġreferred', 'Ġto', 'Ġas', 'ĠAh', 'unts', 'ic', 'Ġare', 'Ġalso', 'Ġnot', 'Ġuniversally', 'Ġagreed', 'Ġupon', 'Ġby', 'Ġauthors', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "input_text = 'Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors.'\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = model.to_tokens(input_text)[:, 1:] # Skip BOS\n",
    "print(tokens.shape, tokens)\n",
    "print(model.tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "\n",
    "# \n",
    "# embeddings_tree = BallTree(W_E.numpy(), leaf_size=1)\n",
    "\n",
    "SOFT_TOKENS = 500\n",
    "MODEL_DIM = 896\n",
    "L2REG = 0.5\n",
    "\n",
    "def exec_model(model, first_tokens_embedding, tokens):\n",
    "    residual, _tks, _spe, _attn = model.input_to_embed(tokens)\n",
    "\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    both = torch.concat([first_tokens_embedding, residual], axis=1)\n",
    "    #print(first_tokens_embedding.shape, residual.shape, both.shape)\n",
    "    return model(both, start_at_layer=0)\n",
    "\n",
    "def predict(model, first_tokens_embedding, num_tokens):\n",
    "    tokens = []\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    for i in range(num_tokens):\n",
    "        toks = torch.tensor(tokens, dtype=torch.long)\n",
    "        residual, _tks, _spe, _attn = model.input_to_embed(toks)\n",
    "        both = torch.concat([first_tokens_embedding.detach(), residual], axis=1)\n",
    "        res = model(both, start_at_layer=0)[0]\n",
    "        next_token = torch.argmax(res, axis=-1)[-1].item()  # Get the last token predicted\n",
    "        tokens.append(next_token)\n",
    "        probs = torch.softmax(res, axis=-1)\n",
    "        #print(model.tokenizer.convert_ids_to_tokens(tokens))\n",
    "        #print(residual.shape, toks.shape, both.shape, res.shape, probs.shape)\n",
    "    \n",
    "    return (model.tokenizer.decode(tokens),\n",
    "            model.tokenizer.convert_ids_to_tokens(tokens),\n",
    "            torch.tensor(tokens).cpu())  # Convert the final token list to a tensor\n",
    "\n",
    "def token_alignment_loss(logits, tokens, first_tokens_embedding, alpha=1.0, beta=0.7, kappa=0.1, gamma=0.1):\n",
    "    # Calculate the cross-entropy loss\n",
    "    def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "    logits = logits[SOFT_TOKENS - 1:-1, :]\n",
    "\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, tokens)\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_loss = l2(first_tokens_embedding)\n",
    "\n",
    "    nn_loss = nearest_neighbour_loss(first_tokens_embedding[0], index, W_E, device=device)\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = nn_loss * ce_loss + kappa * nn_loss + alpha * ce_loss + beta * l2_loss\n",
    "\n",
    "    return total_loss\n",
    "# \n",
    "def nearest_neighbour_loss(x, index, W_E, device):\n",
    "    # x should be of shape (N, 896)\n",
    "    \n",
    "    # Convert the input tensor to numpy and ensure it's on the CPU\n",
    "    x_np = x.detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "    \n",
    "    # Perform the nearest neighbor search using the Faiss index\n",
    "    # Note: Faiss requires the data to be in float32 format\n",
    "    distances, indices = index.search(x_np, k=3)\n",
    "    \n",
    "    # Get the first nearest neighbor index\n",
    "    ind = indices[:, 0].flatten()\n",
    "    \n",
    "    # Fetch the nearest neighbor embeddings from W_E\n",
    "    nearest_neighbour = W_E[ind]\n",
    "    \n",
    "    # Calculate the L2 distance between x and its nearest neighbors\n",
    "    distance = torch.norm(x - nearest_neighbour, p=2, dim=1)\n",
    "    \n",
    "    # Return the sum of the distances\n",
    "    return torch.sum(distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cIo3gGq3An",
    "outputId": "3adf9e89-092c-46a6-8437-08a6a6929c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Step 0, nnloss=197763.5, Maximum Correct=0, Correct=0, Loss=80775.8359375/0, L2=6608.46142578125, LR=[9.989408977717675], Pred='ananlyryness..The. (evidence, the words of the rights of the'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, nnloss=913752.375, Maximum Correct=20, Correct=20, Loss=227413.46875/5, L2=30566.9921875, LR=[9.473915220904217], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "Step 100, nnloss=1035499.4375, Maximum Correct=25, Correct=25, Loss=766335.5625/10, L2=34652.5859375, LR=[8.9850230191884], Pred='Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Vi'\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient computation for prediction\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m         ps \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_tokens_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_corr\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlookahead\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         temp_corr \u001b[38;5;241m=\u001b[39m (ps[\u001b[38;5;241m2\u001b[39m][:\u001b[38;5;28mmin\u001b[39m(last_corr\u001b[38;5;241m+\u001b[39mlookahead, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;241m==\u001b[39m tokens[:, :last_corr\u001b[38;5;241m+\u001b[39mlookahead]\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     61\u001b[0m         max_corr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(temp_corr, max_corr)\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, first_tokens_embedding, num_tokens)\u001b[0m\n\u001b[1;32m     30\u001b[0m both \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([first_tokens_embedding\u001b[38;5;241m.\u001b[39mdetach(), residual], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m res \u001b[38;5;241m=\u001b[39m model(both, start_at_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get the last token predicted\u001b[39;00m\n\u001b[1;32m     33\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(next_token)\n\u001b[1;32m     34\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(res, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "first_tokens_embedding = torch.randn(size=(1, SOFT_TOKENS, MODEL_DIM), dtype=torch.float32, device=device)\n",
    "if device == \"cuda\": \n",
    "    first_tokens_embedding = first_tokens_embedding.cuda()\n",
    "if device == \"mps\":\n",
    "    first_tokens_embedding = first_tokens_embedding.to(torch.float32).to(device)\n",
    "\n",
    "first_tokens_embedding = first_tokens_embedding.requires_grad_(True)\n",
    "\n",
    "num_steps = 5000  # Number of optimization steps\n",
    "# Define the optimizer for the first tokens' embedding\n",
    "optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02 * SOFT_TOKENS)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "last_corr = 0\n",
    "max_corr = 0\n",
    "lossahead = 5 + SOFT_TOKENS * 2\n",
    "lookahead = 20\n",
    "kappa = 0.0\n",
    "alpha = 0.0\n",
    "beta = 1.0\n",
    "for step in range(num_steps + 1):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Replace the first token's embedding in the reg_embeddings tensor\n",
    "    logits = exec_model(model, first_tokens_embedding, tokens)\n",
    "    #print(logits.shape)\n",
    "    # Calculate the loss\n",
    "    # loss = F.cross_entropy(logits.flatten(0, 1)[:last_corr+lossahead], tokens.flatten(0, 1)[:last_corr+lossahead]) + L2REG * l2(first_tokens_embedding)\n",
    "\n",
    "    flattened_logits = logits.flatten(0, 1)[:last_corr+lookahead+SOFT_TOKENS]\n",
    "    flattened_tokens = tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    loss = token_alignment_loss(flattened_logits, flattened_tokens, first_tokens_embedding, alpha=0.0, beta=1.0, kappa=kappa)\n",
    "    \n",
    "    # Backpropagate the loss with retain_graph=True\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the first token's embedding\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    # Print the loss for every 100 steps\n",
    "    print(step)\n",
    "    if step % 50 == 0:\n",
    "\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for prediction\n",
    "\n",
    "            ps = predict(model, first_tokens_embedding, last_corr+lookahead)\n",
    "            temp_corr = (ps[2][:min(last_corr+lookahead, tokens.shape[1])] == tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "            max_corr = max(temp_corr, max_corr)\n",
    "            if temp_corr > last_corr:\n",
    "                last_corr += math.ceil((temp_corr - last_corr) / 4)\n",
    "            if temp_corr > 15:\n",
    "                kappa = 1.0\n",
    "            else:\n",
    "                kappa=0.3\n",
    "            print(f\"\\nStep {step}, nnloss={nearest_neighbour_loss(first_tokens_embedding, index, W_E, device=device)}, Maximum Correct={max_corr}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, L2={l2(first_tokens_embedding).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}\")\n",
    "    \n",
    "\n",
    "    # just comment out this part if you want to not add tokens\n",
    "    # if step % 20 == 0 and step != 0:\n",
    "    #     new_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, 1, MODEL_DIM)), dtype=torch.float32, requires_grad=True).to(device)\n",
    "    #     print(\"ADDING NEW TOK\")\n",
    "    #     first_tokens_embedding = torch.cat([ new_tokens_embedding, first_tokens_embedding], dim=1)\n",
    "    #     first_tokens_embedding = first_tokens_embedding.detach().requires_grad_(True)\n",
    "\n",
    "    #     # Reinitialize the optimizer with the updated embedding\n",
    "    #     optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 896])\n",
      "tensor([[0.0055, 0.0076, 0.0072, 0.0094, 0.0045, 0.0092, 0.0123, 0.0093, 0.0105, 0.0082, 0.0097, 0.0115, 0.0146, 0.0079, 0.0195, 0.0093, 0.1252, 0.0061, 0.0083, 0.0082, 0.0089, 0.0079, 0.0088, 0.0094, 0.0068, 0.0112, 0.0085, 0.0065, 0.0079, 0.0080, 0.0093, 0.0199, 0.0088, 0.0085, 0.0076, 0.0075, 0.0081, 0.0070, 0.0091, 0.0231, 0.0093, 0.0164, 0.0356, 0.0506, 0.3930, 0.0116, 0.0426, 0.1727, 0.0277, 0.0211]], device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_token_embeddings = torch.rand(size=first_tokens_embedding.shape, dtype=torch.float32, device=device)\n",
    "\n",
    "\"\"\"def nearest_neighbour_loss(x, index, W_E, device):\n",
    "    # x should be of shape (N, 896)\n",
    "    \n",
    "    # Convert the input tensor to numpy and ensure it's on the CPU\n",
    "    x_np = x.detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "    \n",
    "    # Perform the nearest neighbor search using the Faiss index\n",
    "    # Note: Faiss requires the data to be in float32 format\n",
    "    distances, indices = index.search(x_np, k=3)\n",
    "    \n",
    "    # Get the first nearest neighbor index\n",
    "    ind = indices[:, 0].flatten()\n",
    "    \n",
    "    # Fetch the nearest neighbor embeddings from W_E\n",
    "    nearest_neighbour = W_E[ind]\n",
    "    \n",
    "    # Calculate the L2 distance between x and its nearest neighbors\n",
    "    distance = torch.norm(x - nearest_neighbour, p=2, dim=1)\n",
    "    \n",
    "    # Return the sum of the distances\n",
    "    return torch.sum(distance)\"\"\" \n",
    "\n",
    "x_np = first_tokens_embedding[0].detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "distances, indices = index.search(x_np, k=3)\n",
    "ind = indices[:, 0].flatten()         \n",
    "# concatenate W_E[ind]\n",
    "nearest_neighbour = W_E[ind].unsqueeze(0)\n",
    "print(nearest_neighbour.shape)\n",
    "\n",
    "# get distance between first_token_embeddings and nearest_neighbour\n",
    "distance = torch.norm(first_tokens_embedding - nearest_neighbour, p=2, dim=2)\n",
    "print(distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauI6iQcA3x5",
    "outputId": "f5904061-a6d0-471f-bbd2-a23a818aea05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'\n",
      "'Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors.'\n"
     ]
    }
   ],
   "source": [
    "# The prediction runs only given the following tensor shaped (1,1,768):\n",
    "# print(first_tokens_embedding)\n",
    "MAGIC = nearest_neighbour\n",
    "# Compare the following:\n",
    "print(repr(predict(model, MAGIC, 90)[0]))\n",
    "print(repr(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQDhlHdYH_oC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
