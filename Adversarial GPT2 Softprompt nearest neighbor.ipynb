{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIr3ZNmedIg4",
    "outputId": "09af3389-9c9b-48f7-bcb1-cffff05f47a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2-0.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## Updated by gavento based on code by Sudarsh\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Load the pre-trained model\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2-0.5B\", device=device)\n",
    "# gemma_model = HookedTransformer.from_pretrained(\"gemma-2b\", device=\"mps\")\n",
    "torch.set_printoptions(threshold=1_000_000)\n",
    "torch.set_printoptions(linewidth=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pynndescent import NNDescent\n",
    "\n",
    "# W_E = model.embed._parameters[\"W_E\"].detach().cpu()\n",
    "# index = NNDescent(W_E)\n",
    "# index.prepare()\n",
    "\n",
    "import faiss\n",
    "\n",
    "resources = faiss.StandardGpuResources()\n",
    "index = faiss.IndexFlatL2(model.embed._parameters[\"W_E\"].shape[1])\n",
    "index = faiss.index_cpu_to_gpu(resources, 0, index)\n",
    "\n",
    "W_E = model.embed._parameters[\"W_E\"].detach()\n",
    "\n",
    "index.add(model.embed._parameters[\"W_E\"].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PR7dgCGc9_u",
    "outputId": "1f665d4e-1728-4829-bd1b-c6103f63748c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74]) tensor([[15220, 13626,   374, 31061,  4221, 50701,   448,  5250,   311,  1128,   279,  5042,  4357, 14590,   419,  1538,    13,   576, 73933,   315, 16366,  3850,  3427,   323, 11401,   301,   438, 59349,  5428,   304,  5411,  7674,   374, 75779,   311,   553,  1846, 11811,   879,  7850,   279, 22240,   429,   807,  1033, 31385,    13,   576, 21551,  9569,   323,   279,  5042, 50823,  5298,   829,   315,   279, 72898, 13862,   311,   438, 16366, 36940,   292,   525,  1083,   537, 60428,  7230,  5193,   553, 12014,    13]], device='cuda:0')\n",
      "['Cons', 'ensus', 'Ġis', 'Ġlacking', 'Ġamong', 'Ġhistorians', 'Ġwith', 'Ġregard', 'Ġto', 'Ġwhat', 'Ġthe', 'Ġactual', 'Ġevents', 'Ġsurrounding', 'Ġthis', 'Ġevent', '.', 'ĠThe', 'Ġportrayal', 'Ġof', 'ĠAh', 'unt', 'isc', 'Ġand', 'ĠVi', 'el', 'Ġas', 'Ġmarty', 'rs', 'Ġin', 'Ġpopular', 'Ġculture', 'Ġis', 'Ġobjected', 'Ġto', 'Ġby', 'Ġthose', 'Ġresearchers', 'Ġwho', 'Ġreject', 'Ġthe', 'Ġnotion', 'Ġthat', 'Ġthey', 'Ġwere', 'Ġmurdered', '.', 'ĠThe', 'Ġethnic', 'Ġidentity', 'Ġand', 'Ġthe', 'Ġactual', 'Ġphon', 'etic', 'Ġname', 'Ġof', 'Ġthe', 'Ġmissionary', 'Ġreferred', 'Ġto', 'Ġas', 'ĠAh', 'unts', 'ic', 'Ġare', 'Ġalso', 'Ġnot', 'Ġuniversally', 'Ġagreed', 'Ġupon', 'Ġby', 'Ġauthors', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "input_text = 'Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors.'\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = model.to_tokens(input_text)[:, 1:] # Skip BOS\n",
    "print(tokens.shape, tokens)\n",
    "print(model.tokenizer.convert_ids_to_tokens(tokens[0]))\n",
    "\n",
    "# \n",
    "# embeddings_tree = BallTree(W_E.numpy(), leaf_size=1)\n",
    "\n",
    "SOFT_TOKENS = 50\n",
    "MODEL_DIM = 896\n",
    "L2REG = 0.5\n",
    "\n",
    "def exec_model(model, first_tokens_embedding, tokens):\n",
    "    residual, _tks, _spe, _attn = model.input_to_embed(tokens)\n",
    "\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    both = torch.concat([first_tokens_embedding, residual], axis=1)\n",
    "    #print(first_tokens_embedding.shape, residual.shape, both.shape)\n",
    "    return model(both, start_at_layer=0)\n",
    "\n",
    "def predict(model, first_tokens_embedding, num_tokens):\n",
    "    tokens = []\n",
    "    skip = first_tokens_embedding.shape[1]\n",
    "    for i in range(num_tokens):\n",
    "        toks = torch.tensor(tokens, dtype=torch.long)\n",
    "        residual, _tks, _spe, _attn = model.input_to_embed(toks)\n",
    "        both = torch.concat([first_tokens_embedding.detach(), residual], axis=1)\n",
    "        res = model(both, start_at_layer=0)[0]\n",
    "        next_token = torch.argmax(res, axis=-1)[-1].item()  # Get the last token predicted\n",
    "        tokens.append(next_token)\n",
    "        probs = torch.softmax(res, axis=-1)\n",
    "        #print(model.tokenizer.convert_ids_to_tokens(tokens))\n",
    "        #print(residual.shape, toks.shape, both.shape, res.shape, probs.shape)\n",
    "    \n",
    "    return (model.tokenizer.decode(tokens),\n",
    "            model.tokenizer.convert_ids_to_tokens(tokens),\n",
    "            torch.tensor(tokens).cpu())  # Convert the final token list to a tensor\n",
    "\n",
    "def token_alignment_loss(logits, tokens, first_tokens_embedding, alpha=1.0, beta=0.7, kappa=0.1, gamma=0.1):\n",
    "    # Calculate the cross-entropy loss\n",
    "    def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "    logits = logits[SOFT_TOKENS - 1:-1, :]\n",
    "\n",
    "\n",
    "    ce_loss = F.cross_entropy(logits, tokens)\n",
    "    \n",
    "    # Calculate the token matching loss\n",
    "    pred_tokens = torch.argmax(logits, dim=-1)\n",
    "    match_loss = (pred_tokens != tokens).float().mean()\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_loss = l2(first_tokens_embedding)\n",
    "\n",
    "    nn_loss = nearest_neighbour_loss(first_tokens_embedding[0], index, W_E, device=device)\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = alpha * match_loss + beta * ce_loss + gamma * l2_loss + kappa * nn_loss\n",
    "\n",
    "    return total_loss\n",
    "# \n",
    "def nearest_neighbour_loss(x, index, W_E, device):\n",
    "    # x should be of shape (N, 896)\n",
    "    \n",
    "    # Convert the input tensor to numpy and ensure it's on the CPU\n",
    "    x_np = x.detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "    \n",
    "    # Perform the nearest neighbor search using the Faiss index\n",
    "    # Note: Faiss requires the data to be in float32 format\n",
    "    distances, indices = index.search(x_np, k=3)\n",
    "    \n",
    "    # Get the first nearest neighbor index\n",
    "    ind = indices[:, 0].flatten()\n",
    "    \n",
    "    # Fetch the nearest neighbor embeddings from W_E\n",
    "    nearest_neighbour = W_E[ind]\n",
    "    \n",
    "    # Calculate the L2 distance between x and its nearest neighbors\n",
    "    distance = torch.norm(x - nearest_neighbour, p=2, dim=1)\n",
    "    \n",
    "    # Return the sum of the distances\n",
    "    return torch.sum(distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cIo3gGq3An",
    "outputId": "3adf9e89-092c-46a6-8437-08a6a6929c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Step 0, nnloss=4111.42236328125, Maximum Correct=0, Correct=0, Loss=774.84033203125/0, L2=138.3819580078125, LR=[0.9989408977717675], Pred=' is a very good way to get a good idea of what the author is trying to say. It'\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "Step 50, nnloss=1375.131591796875, Maximum Correct=1, Correct=1, Loss=214.8732452392578/1, L2=48.134788513183594, LR=[0.9473915220904213], Pred=' Sim based according with with with with with with with with with with with with with with with with with'\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "Step 100, nnloss=1900.218994140625, Maximum Correct=1, Correct=0, Loss=330.33258056640625/1, L2=67.19544982910156, LR=[0.8985023019188398], Pred='                     '\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "\n",
      "Step 150, nnloss=1245.798583984375, Maximum Correct=1, Correct=0, Loss=313.10552978515625/1, L2=45.64154052734375, LR=[0.8521359625132926], Pred='s)的s的s的s的s的s的s的s的s的s的'\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "\n",
      "Step 200, nnloss=1901.970703125, Maximum Correct=1, Correct=0, Loss=430.370361328125/1, L2=72.95826721191406, LR=[0.80816231305999], Pred='\\n                    '\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "\n",
      "Step 250, nnloss=1022.0744018554688, Maximum Correct=1, Correct=0, Loss=334.33758544921875/1, L2=37.30947494506836, LR=[0.7664578811157556], Pred='                     '\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "\n",
      "Step 300, nnloss=3392.0849609375, Maximum Correct=1, Correct=1, Loss=752.1326904296875/1, L2=128.65921020507812, LR=[0.7269055659130281], Pred='nbsp....................'\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "\n",
      "Step 350, nnloss=3122.3740234375, Maximum Correct=1, Correct=0, Loss=638.1068115234375/1, L2=124.60372924804688, LR=[0.6893943095557242], Pred='\\n\\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n'\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "\n",
      "Step 400, nnloss=818.3333129882812, Maximum Correct=1, Correct=0, Loss=398.47271728515625/1, L2=30.5736141204834, LR=[0.6538187851827201], Pred='的的的的的的的的的的的的的的的的的的的的的'\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "\n",
      "Step 450, nnloss=1172.44384765625, Maximum Correct=1, Correct=0, Loss=562.60400390625/1, L2=43.97089767456055, LR=[0.6200791012233539], Pred='\\n      \\n ( ( (\\n ( ( ( (\\n\\n ( (\\n'\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "\n",
      "Step 500, nnloss=1373.775146484375, Maximum Correct=1, Correct=0, Loss=526.4932861328125/1, L2=52.444400787353516, LR=[0.588080520914534], Pred='s)                   '\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "\n",
      "Step 550, nnloss=541.4848022460938, Maximum Correct=1, Correct=0, Loss=352.8724365234375/1, L2=19.37415885925293, LR=[0.5577331962919003], Pred='\\n中文翻译：中文翻译：中文翻译：中文翻译：中文翻译：中文翻译：中文翻译'\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "\n",
      "Step 600, nnloss=2175.6845703125, Maximum Correct=1, Correct=0, Loss=866.0985107421875/1, L2=93.15705108642578, LR=[0.5289519159081053], Pred=' �点点点点点点点点点点点点点点点点点点点点'\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "\n",
      "Step 650, nnloss=3025.85693359375, Maximum Correct=1, Correct=0, Loss=853.0397338867188/1, L2=105.28004455566406, LR=[0.5016558655698555], Pred='\\n \\n\\n\\n\\n \\n \\n           '\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "\n",
      "Step 700, nnloss=1718.707763671875, Maximum Correct=1, Correct=0, Loss=854.5662841796875/1, L2=73.80152130126953, LR=[0.475768401421882], Pred='s 是 は は は は は は は は は'\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "\n",
      "Step 750, nnloss=1382.236083984375, Maximum Correct=1, Correct=0, Loss=788.9727783203125/1, L2=50.25108337402344, LR=[0.45121683474069346], Pred='的)                   '\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "\n",
      "Step 800, nnloss=2129.555908203125, Maximum Correct=1, Correct=0, Loss=970.7797241210938/1, L2=73.28850555419922, LR=[0.4279322278338391], Pred='_的的的的的的的的的的的的的的的的的的的的'\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "\n",
      "Step 850, nnloss=1214.797607421875, Maximum Correct=1, Correct=0, Loss=846.8155517578125/1, L2=45.38456726074219, LR=[0.4058492004715917], Pred='AsO\\nsOsoooooooooooooo'\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "\n",
      "Step 900, nnloss=5950.25390625, Maximum Correct=1, Correct=0, Loss=1787.4630126953125/1, L2=211.41030883789062, LR=[0.3849057463075355], Pred='\\nA. 1\\nB. 2\\nC. 3\\nD. 4\\n'\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "\n",
      "Step 950, nnloss=4218.39111328125, Maximum Correct=1, Correct=0, Loss=1488.0494384765625/1, L2=157.18771362304688, LR=[0.3650430587725925], Pred='\\nDner\\nA\\nDNE\\nA\\nDNE\\nA\\nDNE\\nA\\n'\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "\n",
      "Step 1000, nnloss=1003.1505126953125, Maximum Correct=1, Correct=0, Loss=737.1975708007812/1, L2=49.296058654785156, LR=[0.34620536595361723], Pred='     \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "\n",
      "Step 1050, nnloss=429.49468994140625, Maximum Correct=1, Correct=0, Loss=462.912109375/1, L2=17.134986877441406, LR=[0.32833977399292213], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "\n",
      "Step 1100, nnloss=2237.523681640625, Maximum Correct=1, Correct=0, Loss=984.069091796875/1, L2=86.9530029296875, LR=[0.3113961185690188], Pred=' �理 �理 �理 �理 �理 �理 �理 �理 �理 �理 �'\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "\n",
      "Step 1150, nnloss=1217.9263916015625, Maximum Correct=1, Correct=0, Loss=681.9505615234375/1, L2=53.043701171875, LR=[0.29532682404155136], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "\n",
      "Step 1200, nnloss=453.8878479003906, Maximum Correct=1, Correct=0, Loss=429.4351806640625/1, L2=17.055692672729492, LR=[0.2800867698649178], Pred='语语语语语语语语语语语语语语语语语语语语语'\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "\n",
      "Step 1250, nnloss=820.3121337890625, Maximum Correct=1, Correct=1, Loss=530.3572998046875/1, L2=31.727113723754883, LR=[0.2656331638954883], Pred='.....................'\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "\n",
      "Step 1300, nnloss=909.0844116210938, Maximum Correct=1, Correct=0, Loss=564.1543579101562/1, L2=39.648311614990234, LR=[0.2519254222366804], Pred='Coa)r or a r o r a r o r a r o r a r o'\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "\n",
      "Step 1350, nnloss=458.4884948730469, Maximum Correct=1, Correct=0, Loss=415.3641662597656/1, L2=21.878063201904297, LR=[0.23892505528451313], Pred='l努努nl努努努努努努努努努u努u 的努努'\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "\n",
      "Step 1400, nnloss=1148.280517578125, Maximum Correct=1, Correct=0, Loss=741.6488037109375/1, L2=44.58797836303711, LR=[0.22659555965366984], Pred='\\nth的的的th的确确n\\n\\n\\n\\noon\\n\\noo'\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "\n",
      "Step 1450, nnloss=505.3078308105469, Maximum Correct=1, Correct=0, Loss=565.2841796875/1, L2=19.86954689025879, LR=[0.21490231568061086], Pred=' 2017-07-28 15:30:00\\n'\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "\n",
      "Step 1500, nnloss=251.12850952148438, Maximum Correct=1, Correct=0, Loss=365.8077697753906/1, L2=10.691670417785645, LR=[0.20381249021593947], Pred=' �動動動動動動動動動動動動動動動動動動動動'\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "\n",
      "Step 1550, nnloss=381.21258544921875, Maximum Correct=1, Correct=0, Loss=424.9120788574219/1, L2=16.074066162109375, LR=[0.19329494443307318], Pred='的法法法法法法法法法法法法法法法法法法法法'\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "\n",
      "Step 1600, nnloss=412.42022705078125, Maximum Correct=1, Correct=0, Loss=494.372802734375/1, L2=16.252914428710938, LR=[0.18332014639435895], Pred='))))))))))) () ( ( ( () () ('\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "\n",
      "Step 1650, nnloss=403.30133056640625, Maximum Correct=1, Correct=0, Loss=458.24017333984375/1, L2=14.880240440368652, LR=[0.17386008812912893], Pred=')))))))))))))))))))))'\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "\n",
      "Step 1700, nnloss=992.7655029296875, Maximum Correct=1, Correct=1, Loss=899.3204345703125/1, L2=37.68968963623047, LR=[0.16488820699086365], Pred='分析\\n 1. 1. 1. 1. 1. 1. '\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "\n",
      "Step 1750, nnloss=214.16146850585938, Maximum Correct=1, Correct=0, Loss=319.9466552734375/1, L2=8.408853530883789, LR=[0.15637931107264136], Pred='的consensus的consensus的consensus的consensus的consensus的consensus的consensus'\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "\n",
      "Step 1800, nnloss=255.58399963378906, Maximum Correct=1, Correct=0, Loss=371.197265625/1, L2=9.827156066894531, LR=[0.1483095084714514], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "\n",
      "Step 1850, nnloss=271.3232421875, Maximum Correct=1, Correct=1, Loss=331.9801330566406/1, L2=11.530680656433105, LR=[0.14065614020275394], Pred=',....................'\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "\n",
      "Step 1900, nnloss=282.828125, Maximum Correct=1, Correct=0, Loss=326.6961364746094/1, L2=11.128936767578125, LR=[0.13339771657691848], Pred='\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\nthe\\n'\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "\n",
      "Step 1950, nnloss=159.7080841064453, Maximum Correct=1, Correct=0, Loss=313.387451171875/1, L2=7.395981311798096, LR=[0.12651385685889505], Pred='.allDDDDDDD状状状状状DDD状状DD'\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "\n",
      "Step 2000, nnloss=331.4634094238281, Maximum Correct=1, Correct=0, Loss=430.45977783203125/1, L2=14.064573287963867, LR=[0.11998523204168864], Pred=', can an can 的   can can   can can  how to\\n \\n what'\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "\n",
      "Step 2050, nnloss=937.8418579101562, Maximum Correct=1, Correct=0, Loss=525.1403198242188/1, L2=35.9422607421875, LR=[0.11379351057295405], Pred='conntional 纯conntional 纯conntional 纯'\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "\n",
      "Step 2100, nnloss=821.4954833984375, Maximum Correct=1, Correct=1, Loss=1040.254150390625/1, L2=28.9574031829834, LR=[0.10792130688231627], Pred=')))((((((((.(((....(('\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "\n",
      "Step 2150, nnloss=108.35792541503906, Maximum Correct=1, Correct=0, Loss=213.7493438720703/1, L2=5.341330051422119, LR=[0.10235213256488897], Pred='100000000000000000000'\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "\n",
      "Step 2200, nnloss=308.73675537109375, Maximum Correct=1, Correct=0, Loss=457.75787353515625/1, L2=12.562350273132324, LR=[0.09707035008391998], Pred=\" the ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\"\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "\n",
      "Step 2250, nnloss=386.2556457519531, Maximum Correct=1, Correct=0, Loss=504.9415588378906/1, L2=16.977962493896484, LR=[0.09206112886256695], Pred='\\xa0C\\xa0n\\xa0t\\xa0n\\xa0n\\xa0n\\xa0n\\xa0n\\xa0n\\xa0n\\xa0'\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "\n",
      "Step 2300, nnloss=415.74658203125, Maximum Correct=1, Correct=0, Loss=547.728515625/1, L2=17.30955696105957, LR=[0.08731040364151436], Pred='\\n 1. 1. 1. 1. 1. 1. 1'\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "\n",
      "Step 2350, nnloss=749.7083740234375, Maximum Correct=1, Correct=0, Loss=542.0493774414062/1, L2=27.825550079345703, LR=[0.08280483498550494], Pred='\\n1. 1. 1. 1. 1. 1. 1.'\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "\n",
      "Step 2400, nnloss=107.6791763305664, Maximum Correct=1, Correct=0, Loss=233.9808349609375/1, L2=5.487145900726318, LR=[0.07853177182789371], Pred=' aver Clo aver Clo aver Clo aver Clo aver clo aver clo aver clo aver clo aver clo aver clo aver'\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "\n",
      "Step 2450, nnloss=460.74346923828125, Maximum Correct=1, Correct=0, Loss=474.0470886230469/1, L2=19.40556526184082, LR=[0.07447921594805353], Pred='h strtol眼镜inehoolinl\\nhoolinllalhoollallal'\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "\n",
      "Step 2500, nnloss=784.953369140625, Maximum Correct=1, Correct=0, Loss=750.30517578125/1, L2=28.398937225341797, LR=[0.07063578828188993], Pred='\\n\\n\\n\\n\\n\\n\\n\\n\\nThe\\nThe\\n\\n\\nThe\\nThe\\n\\n\\n'\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "\n",
      "Step 2550, nnloss=206.04315185546875, Maximum Correct=1, Correct=0, Loss=384.2831726074219/1, L2=9.633394241333008, LR=[0.06699069697086919], Pred='ror\\n n\\n 1. 1. 1. 1. 1.'\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "\n",
      "Step 2600, nnloss=87.94120025634766, Maximum Correct=1, Correct=0, Loss=188.59652709960938/1, L2=4.74875545501709, LR=[0.06353370705984493], Pred='\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0'\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "\n",
      "Step 2650, nnloss=281.4677734375, Maximum Correct=1, Correct=0, Loss=309.4864196777344/1, L2=11.035114288330078, LR=[0.060255111758599406], Pred='的的的的的的的的的的的的的的的的的的的的的'\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "\n",
      "Step 2700, nnloss=73.38658905029297, Maximum Correct=1, Correct=0, Loss=182.9286346435547/1, L2=4.571393013000488, LR=[0.057145705186404806], Pred='»\\nTHis is a 100% 100% 100%'\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "\n",
      "Step 2750, nnloss=578.5399169921875, Maximum Correct=1, Correct=0, Loss=544.5986328125/1, L2=21.57274055480957, LR=[0.054196756523075124], Pred='\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "\n",
      "Step 2800, nnloss=86.76606750488281, Maximum Correct=1, Correct=0, Loss=217.19969177246094/1, L2=4.594210624694824, LR=[0.05139998549392789], Pred='] ]] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]'\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "\n",
      "Step 2850, nnloss=22.437908172607422, Maximum Correct=1, Correct=0, Loss=64.53919982910156/1, L2=3.4969162940979004, LR=[0.048747539119820577], Pred='\" 1000000000000000000'\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "\n",
      "Step 2900, nnloss=27.2481746673584, Maximum Correct=1, Correct=0, Loss=74.01393127441406/1, L2=3.563058853149414, LR=[0.046231969666979074], Pred='\\n 1. 2. 3. 4. 5. 6. 7'\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "\n",
      "Step 2950, nnloss=19.349769592285156, Maximum Correct=1, Correct=0, Loss=56.70122528076172/1, L2=3.4880261421203613, LR=[0.04384621373470346], Pred='o»\\n] [Johannes 1999] [Johannes 20'\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "\n",
      "Step 3000, nnloss=27.935768127441406, Maximum Correct=1, Correct=0, Loss=71.65998077392578/1, L2=3.550706148147583, LR=[0.04158357242223292], Pred=' 10000000000000000000'\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "\n",
      "Step 3050, nnloss=33.34727478027344, Maximum Correct=1, Correct=0, Loss=101.52631378173828/1, L2=3.636080265045166, LR=[0.03943769251907984], Pred='it\"‘writ\\'‘writ\\'‘writ\\'‘writ\\'‘writ'\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "\n",
      "Step 3100, nnloss=19.32296371459961, Maximum Correct=1, Correct=1, Loss=55.792972564697266/1, L2=3.4970648288726807, LR=[0.037402548666018845], Pred='\\n  1. 1. 1. 1. 1. 1. '\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "\n",
      "Step 3150, nnloss=16.327159881591797, Maximum Correct=1, Correct=1, Loss=50.97268295288086/1, L2=3.4701013565063477, LR=[0.035472426436640524], Pred='hee\"」] 1. 1.1.1.1.1.1.'\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "\n",
      "Step 3200, nnloss=15.781484603881836, Maximum Correct=1, Correct=0, Loss=47.27734375/1, L2=3.469404935836792, LR=[0.033641906291965194], Pred='\" 1000000000000000000'\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "\n",
      "Step 3250, nnloss=14.569905281066895, Maximum Correct=1, Correct=0, Loss=44.42918395996094/1, L2=3.463128089904785, LR=[0.031905848363063205], Pred='\" - - - - - - - - - - - - - - - - - - - -'\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "\n",
      "Step 3300, nnloss=15.359429359436035, Maximum Correct=1, Correct=0, Loss=47.9206657409668/1, L2=3.473287582397461, LR=[0.030259378018953443], Pred='\" is a word that is used to indicate a specific action or state. It is used to indicate that'\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "\n",
      "Step 3350, nnloss=12.958056449890137, Maximum Correct=1, Correct=0, Loss=43.053985595703125/1, L2=3.4624457359313965, LR=[0.028697872179256335], Pred='he\\n\\n[1] 1.1.1.1.1.1.1.1'\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "\n",
      "Step 3400, nnloss=14.673795700073242, Maximum Correct=1, Correct=0, Loss=41.11050033569336/1, L2=3.468345880508423, LR=[0.02721694633316916], Pred='onderful\" is a word that is used to describe a person who is not a person. It is'\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "\n",
      "Step 3450, nnloss=10.423047065734863, Maximum Correct=2, Correct=2, Loss=38.480403900146484/2, L2=3.4475324153900146, LR=[0.025812442228314577], Pred='et\" is a word that is used to describe a person who is not very active, but who is'\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "\n",
      "Step 3500, nnloss=10.737154006958008, Maximum Correct=2, Correct=2, Loss=38.97450637817383/2, L2=3.4557242393493652, LR=[0.024480416194894102], Pred='et\" is a word that is used to describe a situation in which the water is wet, but not necessarily'\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "\n",
      "Step 3550, nnloss=11.92493724822998, Maximum Correct=2, Correct=0, Loss=38.382713317871094/2, L2=3.4586167335510254, LR=[0.023217128072362328], Pred='\" 10000000000000000000'\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "\n",
      "Step 3600, nnloss=8.564531326293945, Maximum Correct=2, Correct=0, Loss=33.18095397949219/2, L2=3.446647882461548, LR=[0.02201903070752947], Pred='et\" [10] [11] [12] [13] [14]'\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "\n",
      "Step 3650, nnloss=10.00739860534668, Maximum Correct=2, Correct=1, Loss=32.25302505493164/2, L2=3.4491617679595947, LR=[0.02088275999460402], Pred='et\" is a term for a type of water that is not a liquid but a mixture of water and salt'\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "\n",
      "Step 3700, nnloss=10.340167999267578, Maximum Correct=2, Correct=0, Loss=32.33563995361328/2, L2=3.4517645835876465, LR=[0.019805125429209373], Pred='…[…] is a _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _'\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "\n",
      "Step 3750, nnloss=11.526390075683594, Maximum Correct=2, Correct=0, Loss=32.36582946777344/2, L2=3.457353115081787, LR=[0.018783101149851305], Pred='`[. 100000000000000000'\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "\n",
      "Step 3800, nnloss=11.916500091552734, Maximum Correct=2, Correct=1, Loss=32.61257553100586/2, L2=3.461991548538208, LR=[0.01781381744168178], Pred='et\" is a word that means \"to be wet\" in the sense of \"to be wet\" in'\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "\n",
      "Step 3850, nnloss=7.031142234802246, Maximum Correct=2, Correct=1, Loss=26.865341186523438/2, L2=3.439368963241577, LR=[0.016894552678702816], Pred='\\n  1. 1. 1. 1. 1. 1. 1'\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "\n",
      "Step 3900, nnloss=14.886226654052734, Maximum Correct=2, Correct=0, Loss=35.73334884643555/2, L2=3.4784183502197266, LR=[0.016022725681785005], Pred='ritings of the author of the book of the book of the book of the book of the book of the'\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "\n",
      "Step 3950, nnloss=10.07902717590332, Maximum Correct=2, Correct=1, Loss=28.803661346435547/2, L2=3.452716827392578, LR=[0.015195888471043242], Pred='et\" is a word that means \"to be wet\" in the sense of \"to be wet\" in'\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "\n",
      "Step 4000, nnloss=10.933327674865723, Maximum Correct=2, Correct=1, Loss=29.554933547973633/2, L2=3.4614055156707764, LR=[0.014411719392218908], Pred='et\" is the term for the water that is in the water of the body of the person who is being'\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "\n",
      "Step 4050, nnloss=8.46579647064209, Maximum Correct=2, Correct=0, Loss=25.89651870727539/2, L2=3.44671630859375, LR=[0.013668016597768538], Pred=\"' 10000000000000000000\"\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "\n",
      "Step 4100, nnloss=9.992059707641602, Maximum Correct=2, Correct=1, Loss=27.156984329223633/2, L2=3.4547741413116455, LR=[0.01296269186435451], Pred='oman of the time\"» «\"woman of the time\"» «\"woman of the time\"»'\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "\n",
      "Step 4150, nnloss=9.834237098693848, Maximum Correct=2, Correct=0, Loss=28.162900924682617/2, L2=3.456143856048584, LR=[0.012293764729378194], Pred='HAT IS IT?\"\"? 10000000000000'\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "\n",
      "Step 4200, nnloss=8.232081413269043, Maximum Correct=2, Correct=0, Loss=22.995059967041016/2, L2=3.446897029876709, LR=[0.011659356930091564], Pred='ritings of the author of the book of the life of the author of the book of the life of the'\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "\n",
      "Step 4250, nnloss=9.895599365234375, Maximum Correct=2, Correct=1, Loss=24.991233825683594/2, L2=3.4575278759002686, LR=[0.011057687129673092], Pred='onderful\" and \"precise\" depiction of the human figure\" (Harris, 199'\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "\n",
      "Step 4300, nnloss=7.9464216232299805, Maximum Correct=2, Correct=0, Loss=22.687196731567383/2, L2=3.445284366607666, LR=[0.010487065915459335], Pred='et\" in the context of the event, the term was used to refer to the water that was being poured'\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "\n",
      "Step 4350, nnloss=11.385122299194336, Maximum Correct=2, Correct=0, Loss=28.0672664642334/2, L2=3.4578070640563965, LR=[0.009945891055288016], Pred='3c\" 2001\\n  0001\" 0001\"'\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "\n",
      "Step 4400, nnloss=6.5878472328186035, Maximum Correct=2, Correct=0, Loss=22.863914489746094/2, L2=3.4410743713378906, LR=[0.009432642998632798], Pred='ritings\" of the \"Habitation\" of the \"Habitation\" of the \"Hab'\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "\n",
      "Step 4450, nnloss=9.464665412902832, Maximum Correct=2, Correct=0, Loss=23.93716049194336/2, L2=3.4504148960113525, LR=[0.008945880609897728], Pred='onderful\" and \"wonderful\" and \"wonderful\" and \"wonderful\" and'\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "\n",
      "Step 4500, nnloss=7.4654340744018555, Maximum Correct=2, Correct=0, Loss=21.697036743164062/2, L2=3.44277024269104, LR=[0.008484237121890847], Pred='orrying about the future\" and \"worrying about the past\" and \"worrying about the'\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "\n",
      "Step 4550, nnloss=6.749418258666992, Maximum Correct=2, Correct=0, Loss=19.618343353271484/2, L2=3.441964864730835, LR=[0.008046416298114848], Pred='itnessed the death of her daughter, and the death of her son-in-law, and was accused of negligence'\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "\n",
      "Step 4600, nnloss=27.654125213623047, Maximum Correct=2, Correct=0, Loss=49.292724609375/2, L2=3.601548910140991, LR=[0.00763118879309904], Pred='et\"』\\n 1. 1. 1. 1. 1. 1.'\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "\n",
      "Step 4650, nnloss=6.8634209632873535, Maximum Correct=2, Correct=0, Loss=19.873027801513672/2, L2=3.444025993347168, LR=[0.007237388700552811], Pred='oman of the time\"»\\n  1990\\n  1991\\n '\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "\n",
      "Step 4700, nnloss=6.960686206817627, Maximum Correct=2, Correct=0, Loss=17.379810333251953/2, L2=3.441479206085205, LR=[0.006863910279648308], Pred='inkler”(1994–2011), un Americano, qui a avut'\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "\n",
      "Step 4750, nnloss=3.577024459838867, Maximum Correct=2, Correct=1, Loss=17.603099822998047/2, L2=3.436659097671509, LR=[0.006509704850240128], Pred='\\n[1] 1.1.1.1.1.1.1.1.1'\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "\n",
      "Step 4800, nnloss=6.503450870513916, Maximum Correct=2, Correct=0, Loss=19.073379516601562/2, L2=3.4428467750549316, LR=[0.006173777848304145], Pred=\"' 10000000000000000000\"\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "\n",
      "Step 4850, nnloss=6.343796730041504, Maximum Correct=2, Correct=0, Loss=16.59215545654297/2, L2=3.439838409423828, LR=[0.005855186033327607], Pred='itnesses\" is a phrase that means \"those who witness something\" in the context of a document. It'\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "\n",
      "Step 4900, nnloss=6.8439130783081055, Maximum Correct=2, Correct=1, Loss=16.450605392456055/2, L2=3.4420166015625, LR=[0.005553034839809114], Pred='itnessed\" and \"evident\" evidence of the events leading up to the events of this story.'\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "\n",
      "Step 4950, nnloss=6.925395965576172, Maximum Correct=2, Correct=0, Loss=16.629838943481445/2, L2=3.4424524307250977, LR=[0.005266475865431908], Pred='itnessed the death of a child, and was apprehensive about the consequences of his actions, as he was'\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "\n",
      "Step 5000, nnloss=5.540782928466797, Maximum Correct=2, Correct=0, Loss=13.885756492614746/2, L2=3.4379377365112305, LR=[0.004994704488857516], Pred='earing a costume\"»] is a term that is used in academic and public domain scholarship to refer to a'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ecb3ac14100>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "first_tokens_embedding = torch.randn(size=(1, SOFT_TOKENS, MODEL_DIM), dtype=torch.float32, device=device)\n",
    "if device == \"cuda\": \n",
    "    first_tokens_embedding = first_tokens_embedding.cuda()\n",
    "if device == \"mps\":\n",
    "    first_tokens_embedding = first_tokens_embedding.to(torch.float32).to(device)\n",
    "\n",
    "first_tokens_embedding = first_tokens_embedding.requires_grad_(True)\n",
    "\n",
    "num_steps = 5000  # Number of optimization steps\n",
    "# Define the optimizer for the first tokens' embedding\n",
    "optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02 * SOFT_TOKENS)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "def l2(x): return torch.sum(x ** 2) ** 0.5\n",
    "\n",
    "last_corr = 0\n",
    "max_corr = 0\n",
    "lossahead = 5 + SOFT_TOKENS * 2\n",
    "lookahead = 20\n",
    "kappa = 0.0\n",
    "alpha = 0.0\n",
    "beta = 1.0\n",
    "for step in range(num_steps + 1):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Replace the first token's embedding in the reg_embeddings tensor\n",
    "    logits = exec_model(model, first_tokens_embedding, tokens)\n",
    "    #print(logits.shape)\n",
    "    # Calculate the loss\n",
    "    # loss = F.cross_entropy(logits.flatten(0, 1)[:last_corr+lossahead], tokens.flatten(0, 1)[:last_corr+lossahead]) + L2REG * l2(first_tokens_embedding)\n",
    "\n",
    "    flattened_logits = logits.flatten(0, 1)[:last_corr+lookahead+SOFT_TOKENS]\n",
    "    flattened_tokens = tokens.flatten(0, 1)[:last_corr+lookahead]\n",
    "\n",
    "    loss = token_alignment_loss(flattened_logits, flattened_tokens, first_tokens_embedding, alpha=0.0, beta=1.0, kappa=kappa)\n",
    "    \n",
    "    # Backpropagate the loss with retain_graph=True\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the first token's embedding\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "\n",
    "    # Print the loss for every 100 steps\n",
    "    print(step)\n",
    "    if step % 50 == 0:\n",
    "        if last_corr > 30:\n",
    "            kappa = 2.0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for prediction\n",
    "\n",
    "            ps = predict(model, first_tokens_embedding, last_corr+lookahead)\n",
    "            temp_corr = (ps[2][:min(last_corr+lookahead, tokens.shape[1])] == tokens[:, :last_corr+lookahead].cpu()).sum()\n",
    "            max_corr = max(temp_corr, max_corr)\n",
    "            if temp_corr > last_corr:\n",
    "                last_corr += math.ceil((temp_corr - last_corr) / 4)\n",
    "            print(f\"\\nStep {step}, nnloss={nearest_neighbour_loss(first_tokens_embedding, index, W_E, device=device)}, Maximum Correct={max_corr}, Correct={temp_corr}, Loss={loss.item()}/{last_corr}, L2={l2(first_tokens_embedding).detach().cpu()}, LR={lr_scheduler.get_last_lr()}, Pred={ps[0]!r}\")\n",
    "    \n",
    "\n",
    "    # just comment out this part if you want to not add tokens\n",
    "    # if step % 20 == 0 and step != 0:\n",
    "    #     new_tokens_embedding = torch.tensor(np.random.normal(0.0, 768**(-0.5), size=(1, 1, MODEL_DIM)), dtype=torch.float32, requires_grad=True).to(device)\n",
    "    #     print(\"ADDING NEW TOK\")\n",
    "    #     first_tokens_embedding = torch.cat([ new_tokens_embedding, first_tokens_embedding], dim=1)\n",
    "    #     first_tokens_embedding = first_tokens_embedding.detach().requires_grad_(True)\n",
    "\n",
    "    #     # Reinitialize the optimizer with the updated embedding\n",
    "    #     optimizer = torch.optim.Adam([first_tokens_embedding], lr=0.02, amsgrad=True)\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.005 ** (1/num_steps), verbose=True)\n",
    "\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 896])\n",
      "tensor([[0.0055, 0.0076, 0.0072, 0.0094, 0.0045, 0.0092, 0.0123, 0.0093, 0.0105, 0.0082, 0.0097, 0.0115, 0.0146, 0.0079, 0.0195, 0.0093, 0.1252, 0.0061, 0.0083, 0.0082, 0.0089, 0.0079, 0.0088, 0.0094, 0.0068, 0.0112, 0.0085, 0.0065, 0.0079, 0.0080, 0.0093, 0.0199, 0.0088, 0.0085, 0.0076, 0.0075, 0.0081, 0.0070, 0.0091, 0.0231, 0.0093, 0.0164, 0.0356, 0.0506, 0.3930, 0.0116, 0.0426, 0.1727, 0.0277, 0.0211]], device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_token_embeddings = torch.rand(size=first_tokens_embedding.shape, dtype=torch.float32, device=device)\n",
    "\n",
    "\"\"\"def nearest_neighbour_loss(x, index, W_E, device):\n",
    "    # x should be of shape (N, 896)\n",
    "    \n",
    "    # Convert the input tensor to numpy and ensure it's on the CPU\n",
    "    x_np = x.detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "    \n",
    "    # Perform the nearest neighbor search using the Faiss index\n",
    "    # Note: Faiss requires the data to be in float32 format\n",
    "    distances, indices = index.search(x_np, k=3)\n",
    "    \n",
    "    # Get the first nearest neighbor index\n",
    "    ind = indices[:, 0].flatten()\n",
    "    \n",
    "    # Fetch the nearest neighbor embeddings from W_E\n",
    "    nearest_neighbour = W_E[ind]\n",
    "    \n",
    "    # Calculate the L2 distance between x and its nearest neighbors\n",
    "    distance = torch.norm(x - nearest_neighbour, p=2, dim=1)\n",
    "    \n",
    "    # Return the sum of the distances\n",
    "    return torch.sum(distance)\"\"\" \n",
    "\n",
    "x_np = first_tokens_embedding[0].detach().cpu().squeeze(0).numpy().astype('float32')\n",
    "distances, indices = index.search(x_np, k=3)\n",
    "ind = indices[:, 0].flatten()         \n",
    "# concatenate W_E[ind]\n",
    "nearest_neighbour = W_E[ind].unsqueeze(0)\n",
    "print(nearest_neighbour.shape)\n",
    "\n",
    "# get distance between first_token_embeddings and nearest_neighbour\n",
    "distance = torch.norm(first_tokens_embedding - nearest_neighbour, p=2, dim=2)\n",
    "print(distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauI6iQcA3x5",
    "outputId": "f5904061-a6d0-471f-bbd2-a23a818aea05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'\n",
      "'Consensus is lacking among historians with regard to what the actual events surrounding this event. The portrayal of Ahuntisc and Viel as martyrs in popular culture is objected to by those researchers who reject the notion that they were murdered. The ethnic identity and the actual phonetic name of the missionary referred to as Ahuntsic are also not universally agreed upon by authors.'\n"
     ]
    }
   ],
   "source": [
    "# The prediction runs only given the following tensor shaped (1,1,768):\n",
    "# print(first_tokens_embedding)\n",
    "MAGIC = nearest_neighbour\n",
    "# Compare the following:\n",
    "print(repr(predict(model, MAGIC, 90)[0]))\n",
    "print(repr(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQDhlHdYH_oC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
